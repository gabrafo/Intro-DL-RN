{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curso: Redes Neurais e Deep Learning\n",
    "\n",
    "Prof. Denilson Alves Pereira \n",
    "https://sites.google.com/ufla.br/denilsonpereira/ \n",
    "Departamento de Ciência da Computação - \n",
    "Instituto de Ciências Exatas e Tecnológicas - \n",
    "Universidade Federal de Lavras\n",
    "\n",
    "# Atividade Prática 01\n",
    "\n",
    "**Instruções:**\n",
    "1. Siga os passos indicados em cada célula abaixo para completar a atividade.\n",
    "2. Você deve inserir código somente entre as linhas marcadas com **INICIE O CÓDIGO AQUI** e **TERMINE O CÓDIGO AQUI**. Há uma indicação de quantas linhas de código são necessárias.\n",
    "3. Em alguns pontos, confira o resultado esperado conforme marcado com **SAÍDA ESPERADA**.\n",
    "\n",
    "**Tempo estimado para execução**: 1 hora\n",
    "\n",
    "Versão: Junho, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O Problema a ser Resolvido\n",
    "\n",
    "O objetivo da atividade é elaborar uma rede neural para predizer se um paciente tem ou não diabetes, com base nas medidas diagnósticas contidas no *dataset* disponível em https://www.kaggle.com/uciml/pima-indians-diabetes-database.\n",
    "\n",
    "Os dados são de pacientes do sexo feminino, com pelo menos 20 anos de idade. Os atributos das condições médicas incluem o número de gestações que a paciente teve, seu IMC, nível de insulina, idade e outros. A classe a ser predita é o atributo \"Outcome\", cujos valores são 0 (não tem diabetes) ou 1 (tem diabetes). Portanto, é um problema de classificação binária.\n",
    "\n",
    "Você vai praticar as seguintes habilidades:\n",
    "- Efetuar o pré-processamento dos dados, separando-os em conjuntos de treino e teste.\n",
    "- Configurar uma rede neural simples para um problema de classificação binária."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # package for scientific computing\n",
    "import tensorflow as tf  #  package for numerical computation using data flow graphs\n",
    "from tensorflow import keras  # package for deep learning\n",
    "import pandas as pd # package for working with structured data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-Processamentos dos Dados de Treino e de Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read dataset\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "data.head() # display dataset first lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the class from other attributes\n",
    "X = data.drop(\"Outcome\", axis=1)\n",
    "Y = data[\"Outcome\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentação de *train_test_split*: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html <br>\n",
    "A função divide os dados em partições de treino e teste, de acordo com a proporção especificada pelo parâmetro *test_size*. <br>\n",
    "O parâmetro *random_state* é usado para deixar os resultados reproduzíveis para fins de avaliação do exercício."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the dataset for training and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_set_X, test_set_X, train_set_Y, test_set_Y = train_test_split(X, Y, test_size=0.20, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padronize os atributos usando a média e a variância dos dados\n",
    "\n",
    "Dica: use a função *fit_transform*: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "### INICIE O CÓDIGO AQUI ### (2 linhas de código)\n",
    "train_set_X = scaler.fit_transform(train_set_X)\n",
    "test_set_X  = scaler.fit_transform(test_set_X)\n",
    "### TERMINE O CÓDIGO AQUI ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_X:\n",
      " [[ 0.35483802 -0.370418    0.16624635  1.40032403 -0.01279543  0.49863797\n",
      "  -0.61786951  0.01064085]\n",
      " [-0.54794048 -0.55620696  0.9024924   0.96548604  0.40069886  1.70739891\n",
      "  -1.0345765  -0.86049025]]\n",
      "\n",
      "test_set_X:\n",
      " [[-0.83168001 -1.14367855 -0.39723824 -0.57849223 -0.36848171 -0.47210798\n",
      "   0.22800925 -0.84284936]\n",
      " [ 0.8647269   1.8633592   0.67258938  0.00976179  0.72124393  0.58740671\n",
      "   0.24525553  1.28629259]]\n"
     ]
    }
   ],
   "source": [
    "# Checking\n",
    "print(\"train_set_X:\\n\", train_set_X[:2,:])\n",
    "print(\"\\ntest_set_X:\\n\", test_set_X[:2,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAÍDA ESPERADA:** <br>\n",
    "train_set_X: <br>\n",
    " [[ 0.35483802 -0.370418    0.16624635  1.40032403 -0.01279543  0.49863797  -0.61786951  0.01064085] <br>\n",
    " [-0.54794048 -0.55620696  0.9024924   0.96548604  0.40069886  1.70739891  -1.0345765  -0.86049025]] <br>\n",
    "<br>\n",
    "test_set_X: <br>\n",
    "[[-0.83168001 -1.14367855 -0.39723824 -0.57849223 -0.36848171 -0.47210798  0.22800925 -0.84284936] <br>\n",
    " [ 0.8647269   1.8633592   0.67258938  0.00976179  0.72124393  0.58740671  0.24525553  1.28629259]] <br>\n",
    " \n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenha o número de atributos e o número de exemplos de treinamento\n",
    "\n",
    "Dica: use a função *shape*: https://numpy.org/devdocs/reference/generated/numpy.shape.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of attributes: n = 8\n",
      "Number of training examples: m = 614\n",
      "Train set X shape: (614, 8)\n",
      "Train set Y shape: (614,)\n",
      "Test set X shape: (154, 8)\n",
      "Test set Y shape: (154,)\n"
     ]
    }
   ],
   "source": [
    "### INICIE O CÓDIGO AQUI ### (2 linhas de código)\n",
    "n = train_set_X.shape[1]# number of attributes\n",
    "m = train_set_X.shape[0] # number of training examples\n",
    "### TERMINE O CÓDIGO AQUI ###\n",
    "\n",
    "print (\"Number of attributes: n = \" + str(n))\n",
    "print (\"Number of training examples: m = \" + str(m))\n",
    "print (\"Train set X shape: \" + str(train_set_X.shape))\n",
    "print (\"Train set Y shape: \" + str(train_set_Y.shape))\n",
    "print (\"Test set X shape: \" + str(test_set_X.shape))\n",
    "print (\"Test set Y shape: \" + str(test_set_Y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAÍDA ESPERADA**: <br>\n",
    "Number of attributes: n = 8 <br>\n",
    "Number of training examples: m = 614 <br>\n",
    "Train set X shape: (614, 8) <br>\n",
    "Train set Y shape: (614,) <br>\n",
    "Test set X shape: (154, 8) <br>\n",
    "Test set Y shape: (154,) <br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição do Modelo\n",
    "\n",
    "Crie um modelo em Keras com a seguinte configuração:\n",
    "- Camada de entrada: no formato dos dados de entrada do problema\n",
    "- Camada 1: 3 neurônios, função de ativação *Tanh*\n",
    "- Camada 2: 5 neurônios, função de ativação *Tanh*\n",
    "- Camada 3: 3 neurônios, função de ativação *Tanh*\n",
    "- Camada 4 (saída): 1 neurônio, função de ativação *Sigmoid*\n",
    "\n",
    "Dica 1: use a classe *Model*: https://keras.io/api/models/model/ <br>\n",
    "Dica 2: veja as funções de ativação disponíveis: https://keras.io/api/layers/activations/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INICIE O CÓDIGO AQUI ### (6 linhas de código)\n",
    "inputs = keras.Input(shape=(train_set_X.shape[1]))\n",
    "layer1 = keras.layers.Dense(units=3, activation='tanh')(inputs)\n",
    "layer2 = keras.layers.Dense(units=5, activation='tanh')(layer1)\n",
    "layer3 = keras.layers.Dense(units=3, activation='tanh')(layer2)\n",
    "outputs = keras.layers.Dense(units=1, activation='sigmoid')(layer3)\n",
    "model = keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "### TERMINE O CÓDIGO AQUI ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(614, 1)\n"
     ]
    }
   ],
   "source": [
    "# Checking\n",
    "processed_data = model(train_set_X)\n",
    "print(processed_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAÍDA ESPERADA**: <br>\n",
    "(614, 1)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 8)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 27        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 20        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 18        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 69\n",
      "Trainable params: 69\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Prints a summary of the network, showing its architecture and parameters.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAÍDA ESPERADA**: <br>\n",
    "Confira a configuração da rede e o total de parâmetros = 69\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilação do Modelo\n",
    "\n",
    "Compile o model usando os seguintes parâmetros:\n",
    "- Função de perda: mean_absolute_error\n",
    "- Otimizador: RMSprop\n",
    "- Métricas: accuracy, Precision, Recall\n",
    "\n",
    "Dica 1: use a função *compile*: https://keras.io/api/models/model_training_apis/ <br>\n",
    "Dica 2: relação de funções de perda: https://www.tensorflow.org/api_docs/python/tf/keras/losses <br>\n",
    "Dica 3: relação de otimizadores: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers <br>\n",
    "Dica 4: relação de métricas: https://keras.io/api/metrics/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INICIE O CÓDIGO AQUI ### (1 linha de código)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mean_absolute_error\", metrics=[\"accuracy\", keras.metrics.Precision(), keras.metrics.Recall()])\n",
    "### TERMINE O CÓDIGO AQUI ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento do Modelo\n",
    "\n",
    "Ajusta o modelo aos dados de treinamento.\n",
    "Devem ser fornecidos os dados de treinamento, o número de épocas (iterações) e o tamanho do lote (batch). Uma época é composta por uma única passagem por todos os exemplos do conjunto de treino. O tamanho do lote define o número de amostras (exemplos) a serem consideradas pelo modelo antes de atualizar os pesos. Assim, uma época é composta por um ou mais lotes.\n",
    "\n",
    "Efetue o treinamento do modelo usando os seguintes parâmetros:\n",
    "- Tamanho do lote: 64\n",
    "- Número de épocas: 1000\n",
    "\n",
    "Dica: use a função *fit*: https://keras.io/api/models/model_training_apis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "10/10 [==============================] - 1s 739us/step - loss: 0.5031 - accuracy: 0.4756 - precision: 0.3039 - recall: 0.4076\n",
      "Epoch 2/1000\n",
      "10/10 [==============================] - 0s 762us/step - loss: 0.4929 - accuracy: 0.5114 - precision: 0.3308 - recall: 0.4123\n",
      "Epoch 3/1000\n",
      "10/10 [==============================] - 0s 839us/step - loss: 0.4847 - accuracy: 0.5358 - precision: 0.3471 - recall: 0.3981\n",
      "Epoch 4/1000\n",
      "10/10 [==============================] - 0s 744us/step - loss: 0.4765 - accuracy: 0.5619 - precision: 0.3717 - recall: 0.3981\n",
      "Epoch 5/1000\n",
      "10/10 [==============================] - 0s 747us/step - loss: 0.4687 - accuracy: 0.5814 - precision: 0.3905 - recall: 0.3886\n",
      "Epoch 6/1000\n",
      "10/10 [==============================] - 0s 779us/step - loss: 0.4608 - accuracy: 0.6010 - precision: 0.4115 - recall: 0.3744\n",
      "Epoch 7/1000\n",
      "10/10 [==============================] - 0s 747us/step - loss: 0.4529 - accuracy: 0.6238 - precision: 0.4462 - recall: 0.3934\n",
      "Epoch 8/1000\n",
      "10/10 [==============================] - 0s 700us/step - loss: 0.4449 - accuracy: 0.6303 - precision: 0.4551 - recall: 0.3839\n",
      "Epoch 9/1000\n",
      "10/10 [==============================] - 0s 758us/step - loss: 0.4372 - accuracy: 0.6531 - precision: 0.4940 - recall: 0.3886\n",
      "Epoch 10/1000\n",
      "10/10 [==============================] - 0s 688us/step - loss: 0.4296 - accuracy: 0.6743 - precision: 0.5355 - recall: 0.3934\n",
      "Epoch 11/1000\n",
      "10/10 [==============================] - 0s 718us/step - loss: 0.4222 - accuracy: 0.6873 - precision: 0.5638 - recall: 0.3981\n",
      "Epoch 12/1000\n",
      "10/10 [==============================] - 0s 779us/step - loss: 0.4151 - accuracy: 0.6906 - precision: 0.5714 - recall: 0.3981\n",
      "Epoch 13/1000\n",
      "10/10 [==============================] - 0s 675us/step - loss: 0.4083 - accuracy: 0.7003 - precision: 0.5971 - recall: 0.3934\n",
      "Epoch 14/1000\n",
      "10/10 [==============================] - 0s 752us/step - loss: 0.4017 - accuracy: 0.7036 - precision: 0.6058 - recall: 0.3934\n",
      "Epoch 15/1000\n",
      "10/10 [==============================] - 0s 696us/step - loss: 0.3955 - accuracy: 0.7150 - precision: 0.6343 - recall: 0.4028\n",
      "Epoch 16/1000\n",
      "10/10 [==============================] - 0s 739us/step - loss: 0.3897 - accuracy: 0.7182 - precision: 0.6418 - recall: 0.4076\n",
      "Epoch 17/1000\n",
      "10/10 [==============================] - 0s 682us/step - loss: 0.3841 - accuracy: 0.7248 - precision: 0.6591 - recall: 0.4123\n",
      "Epoch 18/1000\n",
      "10/10 [==============================] - 0s 697us/step - loss: 0.3787 - accuracy: 0.7199 - precision: 0.6466 - recall: 0.4076\n",
      "Epoch 19/1000\n",
      "10/10 [==============================] - 0s 808us/step - loss: 0.3735 - accuracy: 0.7264 - precision: 0.6693 - recall: 0.4028\n",
      "Epoch 20/1000\n",
      "10/10 [==============================] - 0s 748us/step - loss: 0.3686 - accuracy: 0.7231 - precision: 0.6614 - recall: 0.3981\n",
      "Epoch 21/1000\n",
      "10/10 [==============================] - 0s 692us/step - loss: 0.3639 - accuracy: 0.7248 - precision: 0.6615 - recall: 0.4076\n",
      "Epoch 22/1000\n",
      "10/10 [==============================] - 0s 686us/step - loss: 0.3594 - accuracy: 0.7248 - precision: 0.6641 - recall: 0.4028\n",
      "Epoch 23/1000\n",
      "10/10 [==============================] - 0s 756us/step - loss: 0.3551 - accuracy: 0.7313 - precision: 0.6825 - recall: 0.4076\n",
      "Epoch 24/1000\n",
      "10/10 [==============================] - 0s 771us/step - loss: 0.3510 - accuracy: 0.7362 - precision: 0.6960 - recall: 0.4123\n",
      "Epoch 25/1000\n",
      "10/10 [==============================] - 0s 764us/step - loss: 0.3471 - accuracy: 0.7378 - precision: 0.7016 - recall: 0.4123\n",
      "Epoch 26/1000\n",
      "10/10 [==============================] - 0s 759us/step - loss: 0.3432 - accuracy: 0.7362 - precision: 0.6960 - recall: 0.4123\n",
      "Epoch 27/1000\n",
      "10/10 [==============================] - 0s 740us/step - loss: 0.3394 - accuracy: 0.7345 - precision: 0.6875 - recall: 0.4171\n",
      "Epoch 28/1000\n",
      "10/10 [==============================] - 0s 748us/step - loss: 0.3359 - accuracy: 0.7329 - precision: 0.6794 - recall: 0.4218\n",
      "Epoch 29/1000\n",
      "10/10 [==============================] - 0s 714us/step - loss: 0.3325 - accuracy: 0.7362 - precision: 0.6842 - recall: 0.4313\n",
      "Epoch 30/1000\n",
      "10/10 [==============================] - 0s 794us/step - loss: 0.3291 - accuracy: 0.7362 - precision: 0.6842 - recall: 0.4313\n",
      "Epoch 31/1000\n",
      "10/10 [==============================] - 0s 712us/step - loss: 0.3258 - accuracy: 0.7378 - precision: 0.6866 - recall: 0.4360\n",
      "Epoch 32/1000\n",
      "10/10 [==============================] - 0s 744us/step - loss: 0.3227 - accuracy: 0.7362 - precision: 0.6815 - recall: 0.4360\n",
      "Epoch 33/1000\n",
      "10/10 [==============================] - 0s 718us/step - loss: 0.3197 - accuracy: 0.7345 - precision: 0.6765 - recall: 0.4360\n",
      "Epoch 34/1000\n",
      "10/10 [==============================] - 0s 765us/step - loss: 0.3168 - accuracy: 0.7394 - precision: 0.6809 - recall: 0.4550\n",
      "Epoch 35/1000\n",
      "10/10 [==============================] - 0s 751us/step - loss: 0.3139 - accuracy: 0.7362 - precision: 0.6763 - recall: 0.4455\n",
      "Epoch 36/1000\n",
      "10/10 [==============================] - 0s 876us/step - loss: 0.3111 - accuracy: 0.7378 - precision: 0.6786 - recall: 0.4502\n",
      "Epoch 37/1000\n",
      "10/10 [==============================] - 0s 828us/step - loss: 0.3084 - accuracy: 0.7394 - precision: 0.6783 - recall: 0.4597\n",
      "Epoch 38/1000\n",
      "10/10 [==============================] - 0s 710us/step - loss: 0.3057 - accuracy: 0.7362 - precision: 0.6690 - recall: 0.4597\n",
      "Epoch 39/1000\n",
      "10/10 [==============================] - 0s 725us/step - loss: 0.3031 - accuracy: 0.7378 - precision: 0.6712 - recall: 0.4645\n",
      "Epoch 40/1000\n",
      "10/10 [==============================] - 0s 759us/step - loss: 0.3006 - accuracy: 0.7427 - precision: 0.6779 - recall: 0.4787\n",
      "Epoch 41/1000\n",
      "10/10 [==============================] - 0s 726us/step - loss: 0.2981 - accuracy: 0.7443 - precision: 0.6824 - recall: 0.4787\n",
      "Epoch 42/1000\n",
      "10/10 [==============================] - 0s 792us/step - loss: 0.2958 - accuracy: 0.7443 - precision: 0.6800 - recall: 0.4834\n",
      "Epoch 43/1000\n",
      "10/10 [==============================] - 0s 802us/step - loss: 0.2935 - accuracy: 0.7476 - precision: 0.6818 - recall: 0.4976\n",
      "Epoch 44/1000\n",
      "10/10 [==============================] - 0s 776us/step - loss: 0.2913 - accuracy: 0.7492 - precision: 0.6839 - recall: 0.5024\n",
      "Epoch 45/1000\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2374 - accuracy: 0.8281 - precision: 0.7778 - recall: 0.666 - 0s 752us/step - loss: 0.2892 - accuracy: 0.7557 - precision: 0.6918 - recall: 0.5213\n",
      "Epoch 46/1000\n",
      "10/10 [==============================] - 0s 724us/step - loss: 0.2872 - accuracy: 0.7573 - precision: 0.6914 - recall: 0.5308\n",
      "Epoch 47/1000\n",
      "10/10 [==============================] - 0s 823us/step - loss: 0.2853 - accuracy: 0.7590 - precision: 0.6933 - recall: 0.5355\n",
      "Epoch 48/1000\n",
      "10/10 [==============================] - 0s 746us/step - loss: 0.2835 - accuracy: 0.7622 - precision: 0.6946 - recall: 0.5498\n",
      "Epoch 49/1000\n",
      "10/10 [==============================] - 0s 750us/step - loss: 0.2817 - accuracy: 0.7671 - precision: 0.7024 - recall: 0.5592\n",
      "Epoch 50/1000\n",
      "10/10 [==============================] - 0s 772us/step - loss: 0.2800 - accuracy: 0.7638 - precision: 0.6964 - recall: 0.5545\n",
      "Epoch 51/1000\n",
      "10/10 [==============================] - 0s 785us/step - loss: 0.2783 - accuracy: 0.7638 - precision: 0.6964 - recall: 0.5545\n",
      "Epoch 52/1000\n",
      "10/10 [==============================] - 0s 753us/step - loss: 0.2767 - accuracy: 0.7622 - precision: 0.6901 - recall: 0.5592\n",
      "Epoch 53/1000\n",
      "10/10 [==============================] - 0s 728us/step - loss: 0.2752 - accuracy: 0.7606 - precision: 0.6839 - recall: 0.5640\n",
      "Epoch 54/1000\n",
      "10/10 [==============================] - 0s 726us/step - loss: 0.2739 - accuracy: 0.7622 - precision: 0.6879 - recall: 0.5640\n",
      "Epoch 55/1000\n",
      "10/10 [==============================] - 0s 742us/step - loss: 0.2723 - accuracy: 0.7622 - precision: 0.6857 - recall: 0.5687\n",
      "Epoch 56/1000\n",
      "10/10 [==============================] - 0s 733us/step - loss: 0.2709 - accuracy: 0.7622 - precision: 0.6857 - recall: 0.5687\n",
      "Epoch 57/1000\n",
      "10/10 [==============================] - 0s 732us/step - loss: 0.2697 - accuracy: 0.7622 - precision: 0.6879 - recall: 0.5640\n",
      "Epoch 58/1000\n",
      "10/10 [==============================] - 0s 735us/step - loss: 0.2683 - accuracy: 0.7655 - precision: 0.6936 - recall: 0.5687\n",
      "Epoch 59/1000\n",
      "10/10 [==============================] - 0s 730us/step - loss: 0.2670 - accuracy: 0.7655 - precision: 0.6914 - recall: 0.5735\n",
      "Epoch 60/1000\n",
      "10/10 [==============================] - 0s 718us/step - loss: 0.2658 - accuracy: 0.7638 - precision: 0.6897 - recall: 0.5687\n",
      "Epoch 61/1000\n",
      "10/10 [==============================] - 0s 770us/step - loss: 0.2647 - accuracy: 0.7655 - precision: 0.6914 - recall: 0.5735\n",
      "Epoch 62/1000\n",
      "10/10 [==============================] - 0s 762us/step - loss: 0.2634 - accuracy: 0.7671 - precision: 0.6932 - recall: 0.5782\n",
      "Epoch 63/1000\n",
      "10/10 [==============================] - 0s 754us/step - loss: 0.2623 - accuracy: 0.7671 - precision: 0.6954 - recall: 0.5735\n",
      "Epoch 64/1000\n",
      "10/10 [==============================] - 0s 697us/step - loss: 0.2610 - accuracy: 0.7687 - precision: 0.6971 - recall: 0.5782\n",
      "Epoch 65/1000\n",
      "10/10 [==============================] - 0s 743us/step - loss: 0.2600 - accuracy: 0.7704 - precision: 0.7011 - recall: 0.5782\n",
      "Epoch 66/1000\n",
      "10/10 [==============================] - 0s 688us/step - loss: 0.2589 - accuracy: 0.7687 - precision: 0.6971 - recall: 0.5782\n",
      "Epoch 67/1000\n",
      "10/10 [==============================] - 0s 770us/step - loss: 0.2577 - accuracy: 0.7687 - precision: 0.6971 - recall: 0.5782\n",
      "Epoch 68/1000\n",
      "10/10 [==============================] - 0s 717us/step - loss: 0.2567 - accuracy: 0.7671 - precision: 0.6954 - recall: 0.5735\n",
      "Epoch 69/1000\n",
      "10/10 [==============================] - 0s 696us/step - loss: 0.2556 - accuracy: 0.7671 - precision: 0.6954 - recall: 0.5735\n",
      "Epoch 70/1000\n",
      "10/10 [==============================] - 0s 753us/step - loss: 0.2546 - accuracy: 0.7704 - precision: 0.7035 - recall: 0.5735\n",
      "Epoch 71/1000\n",
      "10/10 [==============================] - 0s 723us/step - loss: 0.2537 - accuracy: 0.7687 - precision: 0.6994 - recall: 0.5735\n",
      "Epoch 72/1000\n",
      "10/10 [==============================] - 0s 917us/step - loss: 0.2526 - accuracy: 0.7704 - precision: 0.7035 - recall: 0.5735\n",
      "Epoch 73/1000\n",
      "10/10 [==============================] - 0s 749us/step - loss: 0.2517 - accuracy: 0.7704 - precision: 0.7035 - recall: 0.5735\n",
      "Epoch 74/1000\n",
      "10/10 [==============================] - 0s 697us/step - loss: 0.2507 - accuracy: 0.7736 - precision: 0.7118 - recall: 0.5735\n",
      "Epoch 75/1000\n",
      "10/10 [==============================] - 0s 720us/step - loss: 0.2496 - accuracy: 0.7736 - precision: 0.7093 - recall: 0.5782\n",
      "Epoch 76/1000\n",
      "10/10 [==============================] - 0s 757us/step - loss: 0.2487 - accuracy: 0.7736 - precision: 0.7093 - recall: 0.5782\n",
      "Epoch 77/1000\n",
      "10/10 [==============================] - 0s 783us/step - loss: 0.2479 - accuracy: 0.7736 - precision: 0.7069 - recall: 0.5829\n",
      "Epoch 78/1000\n",
      "10/10 [==============================] - 0s 730us/step - loss: 0.2468 - accuracy: 0.7769 - precision: 0.7151 - recall: 0.5829\n",
      "Epoch 79/1000\n",
      "10/10 [==============================] - 0s 754us/step - loss: 0.2461 - accuracy: 0.7752 - precision: 0.7110 - recall: 0.5829\n",
      "Epoch 80/1000\n",
      "10/10 [==============================] - 0s 721us/step - loss: 0.2452 - accuracy: 0.7736 - precision: 0.7069 - recall: 0.5829\n",
      "Epoch 81/1000\n",
      "10/10 [==============================] - 0s 766us/step - loss: 0.2444 - accuracy: 0.7736 - precision: 0.7069 - recall: 0.5829\n",
      "Epoch 82/1000\n",
      "10/10 [==============================] - 0s 739us/step - loss: 0.2437 - accuracy: 0.7785 - precision: 0.7168 - recall: 0.5877\n",
      "Epoch 83/1000\n",
      "10/10 [==============================] - 0s 726us/step - loss: 0.2431 - accuracy: 0.7769 - precision: 0.7126 - recall: 0.5877\n",
      "Epoch 84/1000\n",
      "10/10 [==============================] - 0s 821us/step - loss: 0.2422 - accuracy: 0.7769 - precision: 0.7126 - recall: 0.5877\n",
      "Epoch 85/1000\n",
      "10/10 [==============================] - 0s 756us/step - loss: 0.2415 - accuracy: 0.7785 - precision: 0.7119 - recall: 0.5972\n",
      "Epoch 86/1000\n",
      "10/10 [==============================] - 0s 766us/step - loss: 0.2409 - accuracy: 0.7785 - precision: 0.7119 - recall: 0.5972\n",
      "Epoch 87/1000\n",
      "10/10 [==============================] - 0s 768us/step - loss: 0.2404 - accuracy: 0.7785 - precision: 0.7143 - recall: 0.5924\n",
      "Epoch 88/1000\n",
      "10/10 [==============================] - 0s 743us/step - loss: 0.2398 - accuracy: 0.7752 - precision: 0.7039 - recall: 0.5972\n",
      "Epoch 89/1000\n",
      "10/10 [==============================] - 0s 744us/step - loss: 0.2391 - accuracy: 0.7801 - precision: 0.7159 - recall: 0.5972\n",
      "Epoch 90/1000\n",
      "10/10 [==============================] - 0s 765us/step - loss: 0.2385 - accuracy: 0.7818 - precision: 0.7200 - recall: 0.5972\n",
      "Epoch 91/1000\n",
      "10/10 [==============================] - 0s 809us/step - loss: 0.2381 - accuracy: 0.7818 - precision: 0.7200 - recall: 0.5972\n",
      "Epoch 92/1000\n",
      "10/10 [==============================] - 0s 771us/step - loss: 0.2374 - accuracy: 0.7785 - precision: 0.7119 - recall: 0.5972\n",
      "Epoch 93/1000\n",
      "10/10 [==============================] - 0s 734us/step - loss: 0.2371 - accuracy: 0.7785 - precision: 0.7119 - recall: 0.5972\n",
      "Epoch 94/1000\n",
      "10/10 [==============================] - 0s 741us/step - loss: 0.2365 - accuracy: 0.7785 - precision: 0.7119 - recall: 0.5972\n",
      "Epoch 95/1000\n",
      "10/10 [==============================] - 0s 765us/step - loss: 0.2361 - accuracy: 0.7769 - precision: 0.7079 - recall: 0.5972\n",
      "Epoch 96/1000\n",
      "10/10 [==============================] - 0s 719us/step - loss: 0.2355 - accuracy: 0.7785 - precision: 0.7119 - recall: 0.5972\n",
      "Epoch 97/1000\n",
      "10/10 [==============================] - 0s 713us/step - loss: 0.2350 - accuracy: 0.7769 - precision: 0.7079 - recall: 0.5972\n",
      "Epoch 98/1000\n",
      "10/10 [==============================] - 0s 703us/step - loss: 0.2347 - accuracy: 0.7752 - precision: 0.7062 - recall: 0.5924\n",
      "Epoch 99/1000\n",
      "10/10 [==============================] - 0s 705us/step - loss: 0.2342 - accuracy: 0.7785 - precision: 0.7095 - recall: 0.6019\n",
      "Epoch 100/1000\n",
      "10/10 [==============================] - 0s 711us/step - loss: 0.2339 - accuracy: 0.7769 - precision: 0.7079 - recall: 0.5972\n",
      "Epoch 101/1000\n",
      "10/10 [==============================] - 0s 789us/step - loss: 0.2333 - accuracy: 0.7801 - precision: 0.7111 - recall: 0.6066\n",
      "Epoch 102/1000\n",
      "10/10 [==============================] - 0s 761us/step - loss: 0.2329 - accuracy: 0.7801 - precision: 0.7111 - recall: 0.6066\n",
      "Epoch 103/1000\n",
      "10/10 [==============================] - 0s 735us/step - loss: 0.2326 - accuracy: 0.7801 - precision: 0.7111 - recall: 0.6066\n",
      "Epoch 104/1000\n",
      "10/10 [==============================] - 0s 775us/step - loss: 0.2323 - accuracy: 0.7801 - precision: 0.7088 - recall: 0.6114\n",
      "Epoch 105/1000\n",
      "10/10 [==============================] - 0s 742us/step - loss: 0.2317 - accuracy: 0.7801 - precision: 0.7111 - recall: 0.6066\n",
      "Epoch 106/1000\n",
      "10/10 [==============================] - 0s 739us/step - loss: 0.2313 - accuracy: 0.7801 - precision: 0.7088 - recall: 0.6114\n",
      "Epoch 107/1000\n",
      "10/10 [==============================] - 0s 716us/step - loss: 0.2311 - accuracy: 0.7818 - precision: 0.7104 - recall: 0.6161\n",
      "Epoch 108/1000\n",
      "10/10 [==============================] - 0s 749us/step - loss: 0.2305 - accuracy: 0.7834 - precision: 0.7143 - recall: 0.6161\n",
      "Epoch 109/1000\n",
      "10/10 [==============================] - 0s 722us/step - loss: 0.2304 - accuracy: 0.7818 - precision: 0.7127 - recall: 0.6114\n",
      "Epoch 110/1000\n",
      "10/10 [==============================] - 0s 809us/step - loss: 0.2301 - accuracy: 0.7834 - precision: 0.7143 - recall: 0.6161\n",
      "Epoch 111/1000\n",
      "10/10 [==============================] - 0s 779us/step - loss: 0.2296 - accuracy: 0.7818 - precision: 0.7127 - recall: 0.6114\n",
      "Epoch 112/1000\n",
      "10/10 [==============================] - 0s 735us/step - loss: 0.2292 - accuracy: 0.7850 - precision: 0.7158 - recall: 0.6209\n",
      "Epoch 113/1000\n",
      "10/10 [==============================] - 0s 785us/step - loss: 0.2289 - accuracy: 0.7850 - precision: 0.7158 - recall: 0.6209\n",
      "Epoch 114/1000\n",
      "10/10 [==============================] - 0s 716us/step - loss: 0.2288 - accuracy: 0.7850 - precision: 0.7158 - recall: 0.6209\n",
      "Epoch 115/1000\n",
      "10/10 [==============================] - 0s 764us/step - loss: 0.2283 - accuracy: 0.7834 - precision: 0.7120 - recall: 0.6209\n",
      "Epoch 116/1000\n",
      "10/10 [==============================] - 0s 732us/step - loss: 0.2280 - accuracy: 0.7850 - precision: 0.7158 - recall: 0.6209\n",
      "Epoch 117/1000\n",
      "10/10 [==============================] - 0s 738us/step - loss: 0.2278 - accuracy: 0.7866 - precision: 0.7174 - recall: 0.6256\n",
      "Epoch 118/1000\n",
      "10/10 [==============================] - 0s 745us/step - loss: 0.2276 - accuracy: 0.7866 - precision: 0.7174 - recall: 0.6256\n",
      "Epoch 119/1000\n",
      "10/10 [==============================] - 0s 780us/step - loss: 0.2271 - accuracy: 0.7866 - precision: 0.7174 - recall: 0.6256\n",
      "Epoch 120/1000\n",
      "10/10 [==============================] - 0s 709us/step - loss: 0.2268 - accuracy: 0.7850 - precision: 0.7158 - recall: 0.6209\n",
      "Epoch 121/1000\n",
      "10/10 [==============================] - 0s 744us/step - loss: 0.2267 - accuracy: 0.7866 - precision: 0.7198 - recall: 0.6209\n",
      "Epoch 122/1000\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.2265 - accuracy: 0.7850 - precision: 0.7158 - recall: 0.6209\n",
      "Epoch 123/1000\n",
      "10/10 [==============================] - 0s 787us/step - loss: 0.2262 - accuracy: 0.7883 - precision: 0.7213 - recall: 0.6256\n",
      "Epoch 124/1000\n",
      "10/10 [==============================] - 0s 733us/step - loss: 0.2259 - accuracy: 0.7866 - precision: 0.7198 - recall: 0.6209\n",
      "Epoch 125/1000\n",
      "10/10 [==============================] - 0s 738us/step - loss: 0.2256 - accuracy: 0.7883 - precision: 0.7213 - recall: 0.6256\n",
      "Epoch 126/1000\n",
      "10/10 [==============================] - 0s 726us/step - loss: 0.2254 - accuracy: 0.7883 - precision: 0.7213 - recall: 0.6256\n",
      "Epoch 127/1000\n",
      "10/10 [==============================] - 0s 761us/step - loss: 0.2254 - accuracy: 0.7883 - precision: 0.7213 - recall: 0.6256\n",
      "Epoch 128/1000\n",
      "10/10 [==============================] - 0s 757us/step - loss: 0.2249 - accuracy: 0.7883 - precision: 0.7213 - recall: 0.6256\n",
      "Epoch 129/1000\n",
      "10/10 [==============================] - 0s 709us/step - loss: 0.2246 - accuracy: 0.7899 - precision: 0.7228 - recall: 0.6303\n",
      "Epoch 130/1000\n",
      "10/10 [==============================] - 0s 698us/step - loss: 0.2242 - accuracy: 0.7883 - precision: 0.7213 - recall: 0.6256\n",
      "Epoch 131/1000\n",
      "10/10 [==============================] - 0s 745us/step - loss: 0.2243 - accuracy: 0.7883 - precision: 0.7213 - recall: 0.6256\n",
      "Epoch 132/1000\n",
      "10/10 [==============================] - 0s 735us/step - loss: 0.2241 - accuracy: 0.7899 - precision: 0.7228 - recall: 0.6303\n",
      "Epoch 133/1000\n",
      "10/10 [==============================] - 0s 714us/step - loss: 0.2237 - accuracy: 0.7883 - precision: 0.7213 - recall: 0.6256\n",
      "Epoch 134/1000\n",
      "10/10 [==============================] - 0s 738us/step - loss: 0.2236 - accuracy: 0.7866 - precision: 0.7198 - recall: 0.6209\n",
      "Epoch 135/1000\n",
      "10/10 [==============================] - 0s 866us/step - loss: 0.2234 - accuracy: 0.7899 - precision: 0.7228 - recall: 0.6303\n",
      "Epoch 136/1000\n",
      "10/10 [==============================] - 0s 756us/step - loss: 0.2234 - accuracy: 0.7883 - precision: 0.7213 - recall: 0.6256\n",
      "Epoch 137/1000\n",
      "10/10 [==============================] - 0s 779us/step - loss: 0.2229 - accuracy: 0.7883 - precision: 0.7213 - recall: 0.6256\n",
      "Epoch 138/1000\n",
      "10/10 [==============================] - 0s 847us/step - loss: 0.2228 - accuracy: 0.7883 - precision: 0.7213 - recall: 0.6256\n",
      "Epoch 139/1000\n",
      "10/10 [==============================] - 0s 724us/step - loss: 0.2226 - accuracy: 0.7883 - precision: 0.7213 - recall: 0.6256\n",
      "Epoch 140/1000\n",
      "10/10 [==============================] - 0s 756us/step - loss: 0.2222 - accuracy: 0.7883 - precision: 0.7213 - recall: 0.6256\n",
      "Epoch 141/1000\n",
      "10/10 [==============================] - 0s 784us/step - loss: 0.2223 - accuracy: 0.7883 - precision: 0.7213 - recall: 0.6256\n",
      "Epoch 142/1000\n",
      "10/10 [==============================] - 0s 760us/step - loss: 0.2220 - accuracy: 0.7883 - precision: 0.7213 - recall: 0.6256\n",
      "Epoch 143/1000\n",
      "10/10 [==============================] - 0s 764us/step - loss: 0.2219 - accuracy: 0.7899 - precision: 0.7228 - recall: 0.6303\n",
      "Epoch 144/1000\n",
      "10/10 [==============================] - 0s 764us/step - loss: 0.2217 - accuracy: 0.7899 - precision: 0.7228 - recall: 0.6303\n",
      "Epoch 145/1000\n",
      "10/10 [==============================] - 0s 794us/step - loss: 0.2215 - accuracy: 0.7883 - precision: 0.7213 - recall: 0.6256\n",
      "Epoch 146/1000\n",
      "10/10 [==============================] - 0s 744us/step - loss: 0.2212 - accuracy: 0.7899 - precision: 0.7228 - recall: 0.6303\n",
      "Epoch 147/1000\n",
      "10/10 [==============================] - 0s 745us/step - loss: 0.2212 - accuracy: 0.7899 - precision: 0.7228 - recall: 0.6303\n",
      "Epoch 148/1000\n",
      "10/10 [==============================] - 0s 753us/step - loss: 0.2209 - accuracy: 0.7883 - precision: 0.7213 - recall: 0.6256\n",
      "Epoch 149/1000\n",
      "10/10 [==============================] - 0s 752us/step - loss: 0.2209 - accuracy: 0.7899 - precision: 0.7228 - recall: 0.6303\n",
      "Epoch 150/1000\n",
      "10/10 [==============================] - 0s 730us/step - loss: 0.2206 - accuracy: 0.7899 - precision: 0.7228 - recall: 0.6303\n",
      "Epoch 151/1000\n",
      "10/10 [==============================] - 0s 713us/step - loss: 0.2204 - accuracy: 0.7899 - precision: 0.7228 - recall: 0.6303\n",
      "Epoch 152/1000\n",
      "10/10 [==============================] - 0s 738us/step - loss: 0.2202 - accuracy: 0.7899 - precision: 0.7228 - recall: 0.6303\n",
      "Epoch 153/1000\n",
      "10/10 [==============================] - 0s 714us/step - loss: 0.2202 - accuracy: 0.7899 - precision: 0.7228 - recall: 0.6303\n",
      "Epoch 154/1000\n",
      "10/10 [==============================] - 0s 729us/step - loss: 0.2201 - accuracy: 0.7883 - precision: 0.7213 - recall: 0.6256\n",
      "Epoch 155/1000\n",
      "10/10 [==============================] - 0s 724us/step - loss: 0.2198 - accuracy: 0.7883 - precision: 0.7213 - recall: 0.6256\n",
      "Epoch 156/1000\n",
      "10/10 [==============================] - 0s 746us/step - loss: 0.2196 - accuracy: 0.7899 - precision: 0.7228 - recall: 0.6303\n",
      "Epoch 157/1000\n",
      "10/10 [==============================] - 0s 767us/step - loss: 0.2195 - accuracy: 0.7899 - precision: 0.7228 - recall: 0.6303\n",
      "Epoch 158/1000\n",
      "10/10 [==============================] - 0s 797us/step - loss: 0.2193 - accuracy: 0.7883 - precision: 0.7213 - recall: 0.6256\n",
      "Epoch 159/1000\n",
      "10/10 [==============================] - 0s 753us/step - loss: 0.2197 - accuracy: 0.7899 - precision: 0.7228 - recall: 0.6303\n",
      "Epoch 160/1000\n",
      "10/10 [==============================] - 0s 771us/step - loss: 0.2190 - accuracy: 0.7899 - precision: 0.7228 - recall: 0.6303\n",
      "Epoch 161/1000\n",
      "10/10 [==============================] - 0s 772us/step - loss: 0.2189 - accuracy: 0.7899 - precision: 0.7228 - recall: 0.6303\n",
      "Epoch 162/1000\n",
      "10/10 [==============================] - 0s 748us/step - loss: 0.2188 - accuracy: 0.7899 - precision: 0.7228 - recall: 0.6303\n",
      "Epoch 163/1000\n",
      "10/10 [==============================] - 0s 773us/step - loss: 0.2186 - accuracy: 0.7899 - precision: 0.7228 - recall: 0.6303\n",
      "Epoch 164/1000\n",
      "10/10 [==============================] - 0s 727us/step - loss: 0.2187 - accuracy: 0.7899 - precision: 0.7228 - recall: 0.6303\n",
      "Epoch 165/1000\n",
      "10/10 [==============================] - 0s 779us/step - loss: 0.2183 - accuracy: 0.7899 - precision: 0.7228 - recall: 0.6303\n",
      "Epoch 166/1000\n",
      "10/10 [==============================] - 0s 755us/step - loss: 0.2183 - accuracy: 0.7915 - precision: 0.7243 - recall: 0.6351\n",
      "Epoch 167/1000\n",
      "10/10 [==============================] - 0s 785us/step - loss: 0.2182 - accuracy: 0.7883 - precision: 0.7213 - recall: 0.6256\n",
      "Epoch 168/1000\n",
      "10/10 [==============================] - 0s 687us/step - loss: 0.2182 - accuracy: 0.7883 - precision: 0.7213 - recall: 0.6256\n",
      "Epoch 169/1000\n",
      "10/10 [==============================] - 0s 748us/step - loss: 0.2177 - accuracy: 0.7899 - precision: 0.7204 - recall: 0.6351\n",
      "Epoch 170/1000\n",
      "10/10 [==============================] - 0s 757us/step - loss: 0.2177 - accuracy: 0.7915 - precision: 0.7243 - recall: 0.6351\n",
      "Epoch 171/1000\n",
      "10/10 [==============================] - 0s 754us/step - loss: 0.2176 - accuracy: 0.7915 - precision: 0.7243 - recall: 0.6351\n",
      "Epoch 172/1000\n",
      "10/10 [==============================] - 0s 750us/step - loss: 0.2175 - accuracy: 0.7899 - precision: 0.7204 - recall: 0.6351\n",
      "Epoch 173/1000\n",
      "10/10 [==============================] - 0s 766us/step - loss: 0.2174 - accuracy: 0.7915 - precision: 0.7243 - recall: 0.6351\n",
      "Epoch 174/1000\n",
      "10/10 [==============================] - 0s 749us/step - loss: 0.2173 - accuracy: 0.7915 - precision: 0.7243 - recall: 0.6351\n",
      "Epoch 175/1000\n",
      "10/10 [==============================] - 0s 743us/step - loss: 0.2171 - accuracy: 0.7915 - precision: 0.7243 - recall: 0.6351\n",
      "Epoch 176/1000\n",
      "10/10 [==============================] - 0s 714us/step - loss: 0.2173 - accuracy: 0.7915 - precision: 0.7243 - recall: 0.6351\n",
      "Epoch 177/1000\n",
      "10/10 [==============================] - 0s 768us/step - loss: 0.2168 - accuracy: 0.7915 - precision: 0.7243 - recall: 0.6351\n",
      "Epoch 178/1000\n",
      "10/10 [==============================] - 0s 760us/step - loss: 0.2166 - accuracy: 0.7915 - precision: 0.7243 - recall: 0.6351\n",
      "Epoch 179/1000\n",
      "10/10 [==============================] - 0s 706us/step - loss: 0.2167 - accuracy: 0.7915 - precision: 0.7243 - recall: 0.6351\n",
      "Epoch 180/1000\n",
      "10/10 [==============================] - 0s 785us/step - loss: 0.2167 - accuracy: 0.7899 - precision: 0.7228 - recall: 0.6303\n",
      "Epoch 181/1000\n",
      "10/10 [==============================] - 0s 768us/step - loss: 0.2162 - accuracy: 0.7915 - precision: 0.7243 - recall: 0.6351\n",
      "Epoch 182/1000\n",
      "10/10 [==============================] - 0s 774us/step - loss: 0.2164 - accuracy: 0.7915 - precision: 0.7243 - recall: 0.6351\n",
      "Epoch 183/1000\n",
      "10/10 [==============================] - 0s 780us/step - loss: 0.2162 - accuracy: 0.7915 - precision: 0.7243 - recall: 0.6351\n",
      "Epoch 184/1000\n",
      "10/10 [==============================] - 0s 711us/step - loss: 0.2162 - accuracy: 0.7915 - precision: 0.7243 - recall: 0.6351\n",
      "Epoch 185/1000\n",
      "10/10 [==============================] - 0s 743us/step - loss: 0.2160 - accuracy: 0.7899 - precision: 0.7204 - recall: 0.6351\n",
      "Epoch 186/1000\n",
      "10/10 [==============================] - 0s 731us/step - loss: 0.2160 - accuracy: 0.7915 - precision: 0.7243 - recall: 0.6351\n",
      "Epoch 187/1000\n",
      "10/10 [==============================] - 0s 705us/step - loss: 0.2158 - accuracy: 0.7899 - precision: 0.7204 - recall: 0.6351\n",
      "Epoch 188/1000\n",
      "10/10 [==============================] - 0s 697us/step - loss: 0.2156 - accuracy: 0.7915 - precision: 0.7243 - recall: 0.6351\n",
      "Epoch 189/1000\n",
      "10/10 [==============================] - 0s 695us/step - loss: 0.2154 - accuracy: 0.7899 - precision: 0.7204 - recall: 0.6351\n",
      "Epoch 190/1000\n",
      "10/10 [==============================] - 0s 739us/step - loss: 0.2156 - accuracy: 0.7915 - precision: 0.7243 - recall: 0.6351\n",
      "Epoch 191/1000\n",
      "10/10 [==============================] - 0s 759us/step - loss: 0.2153 - accuracy: 0.7932 - precision: 0.7283 - recall: 0.6351\n",
      "Epoch 192/1000\n",
      "10/10 [==============================] - 0s 753us/step - loss: 0.2154 - accuracy: 0.7932 - precision: 0.7283 - recall: 0.6351\n",
      "Epoch 193/1000\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3100 - accuracy: 0.6875 - precision: 0.7143 - recall: 0.517 - 0s 745us/step - loss: 0.2152 - accuracy: 0.7932 - precision: 0.7283 - recall: 0.6351\n",
      "Epoch 194/1000\n",
      "10/10 [==============================] - 0s 704us/step - loss: 0.2150 - accuracy: 0.7932 - precision: 0.7283 - recall: 0.6351\n",
      "Epoch 195/1000\n",
      "10/10 [==============================] - 0s 774us/step - loss: 0.2149 - accuracy: 0.7899 - precision: 0.7204 - recall: 0.6351\n",
      "Epoch 196/1000\n",
      "10/10 [==============================] - 0s 726us/step - loss: 0.2149 - accuracy: 0.7915 - precision: 0.7243 - recall: 0.6351\n",
      "Epoch 197/1000\n",
      "10/10 [==============================] - 0s 724us/step - loss: 0.2147 - accuracy: 0.7915 - precision: 0.7243 - recall: 0.6351\n",
      "Epoch 198/1000\n",
      "10/10 [==============================] - 0s 769us/step - loss: 0.2145 - accuracy: 0.7932 - precision: 0.7283 - recall: 0.6351\n",
      "Epoch 199/1000\n",
      "10/10 [==============================] - 0s 735us/step - loss: 0.2146 - accuracy: 0.7915 - precision: 0.7243 - recall: 0.6351\n",
      "Epoch 200/1000\n",
      "10/10 [==============================] - 0s 777us/step - loss: 0.2144 - accuracy: 0.7932 - precision: 0.7283 - recall: 0.6351\n",
      "Epoch 201/1000\n",
      "10/10 [==============================] - 0s 771us/step - loss: 0.2142 - accuracy: 0.7948 - precision: 0.7322 - recall: 0.6351\n",
      "Epoch 202/1000\n",
      "10/10 [==============================] - 0s 739us/step - loss: 0.2140 - accuracy: 0.7948 - precision: 0.7322 - recall: 0.6351\n",
      "Epoch 203/1000\n",
      "10/10 [==============================] - 0s 819us/step - loss: 0.2140 - accuracy: 0.7948 - precision: 0.7322 - recall: 0.6351\n",
      "Epoch 204/1000\n",
      "10/10 [==============================] - 0s 762us/step - loss: 0.2141 - accuracy: 0.7948 - precision: 0.7322 - recall: 0.6351\n",
      "Epoch 205/1000\n",
      "10/10 [==============================] - 0s 748us/step - loss: 0.2138 - accuracy: 0.7948 - precision: 0.7322 - recall: 0.6351\n",
      "Epoch 206/1000\n",
      "10/10 [==============================] - 0s 682us/step - loss: 0.2137 - accuracy: 0.7948 - precision: 0.7322 - recall: 0.6351\n",
      "Epoch 207/1000\n",
      "10/10 [==============================] - 0s 744us/step - loss: 0.2135 - accuracy: 0.7948 - precision: 0.7322 - recall: 0.6351\n",
      "Epoch 208/1000\n",
      "10/10 [==============================] - 0s 786us/step - loss: 0.2135 - accuracy: 0.7948 - precision: 0.7322 - recall: 0.6351\n",
      "Epoch 209/1000\n",
      "10/10 [==============================] - 0s 730us/step - loss: 0.2132 - accuracy: 0.7948 - precision: 0.7322 - recall: 0.6351\n",
      "Epoch 210/1000\n",
      "10/10 [==============================] - 0s 721us/step - loss: 0.2132 - accuracy: 0.7948 - precision: 0.7322 - recall: 0.6351\n",
      "Epoch 211/1000\n",
      "10/10 [==============================] - 0s 749us/step - loss: 0.2133 - accuracy: 0.7948 - precision: 0.7322 - recall: 0.6351\n",
      "Epoch 212/1000\n",
      "10/10 [==============================] - 0s 751us/step - loss: 0.2130 - accuracy: 0.7948 - precision: 0.7322 - recall: 0.6351\n",
      "Epoch 213/1000\n",
      "10/10 [==============================] - 0s 741us/step - loss: 0.2131 - accuracy: 0.7948 - precision: 0.7322 - recall: 0.6351\n",
      "Epoch 214/1000\n",
      "10/10 [==============================] - 0s 739us/step - loss: 0.2125 - accuracy: 0.7948 - precision: 0.7322 - recall: 0.6351\n",
      "Epoch 215/1000\n",
      "10/10 [==============================] - 0s 741us/step - loss: 0.2130 - accuracy: 0.7948 - precision: 0.7322 - recall: 0.6351\n",
      "Epoch 216/1000\n",
      "10/10 [==============================] - 0s 775us/step - loss: 0.2125 - accuracy: 0.7948 - precision: 0.7322 - recall: 0.6351\n",
      "Epoch 217/1000\n",
      "10/10 [==============================] - 0s 774us/step - loss: 0.2124 - accuracy: 0.7948 - precision: 0.7322 - recall: 0.6351\n",
      "Epoch 218/1000\n",
      "10/10 [==============================] - 0s 722us/step - loss: 0.2127 - accuracy: 0.7948 - precision: 0.7322 - recall: 0.6351\n",
      "Epoch 219/1000\n",
      "10/10 [==============================] - 0s 748us/step - loss: 0.2124 - accuracy: 0.7948 - precision: 0.7322 - recall: 0.6351\n",
      "Epoch 220/1000\n",
      "10/10 [==============================] - 0s 748us/step - loss: 0.2123 - accuracy: 0.7948 - precision: 0.7322 - recall: 0.6351\n",
      "Epoch 221/1000\n",
      "10/10 [==============================] - 0s 786us/step - loss: 0.2124 - accuracy: 0.7948 - precision: 0.7322 - recall: 0.6351\n",
      "Epoch 222/1000\n",
      "10/10 [==============================] - 0s 744us/step - loss: 0.2122 - accuracy: 0.7948 - precision: 0.7322 - recall: 0.6351\n",
      "Epoch 223/1000\n",
      "10/10 [==============================] - 0s 798us/step - loss: 0.2120 - accuracy: 0.7948 - precision: 0.7322 - recall: 0.6351\n",
      "Epoch 224/1000\n",
      "10/10 [==============================] - 0s 750us/step - loss: 0.2120 - accuracy: 0.7948 - precision: 0.7322 - recall: 0.6351\n",
      "Epoch 225/1000\n",
      "10/10 [==============================] - 0s 709us/step - loss: 0.2120 - accuracy: 0.7948 - precision: 0.7322 - recall: 0.6351\n",
      "Epoch 226/1000\n",
      "10/10 [==============================] - 0s 740us/step - loss: 0.2120 - accuracy: 0.7948 - precision: 0.7322 - recall: 0.6351\n",
      "Epoch 227/1000\n",
      "10/10 [==============================] - 0s 770us/step - loss: 0.2118 - accuracy: 0.7948 - precision: 0.7322 - recall: 0.6351\n",
      "Epoch 228/1000\n",
      "10/10 [==============================] - 0s 801us/step - loss: 0.2116 - accuracy: 0.7964 - precision: 0.7363 - recall: 0.6351\n",
      "Epoch 229/1000\n",
      "10/10 [==============================] - 0s 765us/step - loss: 0.2115 - accuracy: 0.7948 - precision: 0.7322 - recall: 0.6351\n",
      "Epoch 230/1000\n",
      "10/10 [==============================] - 0s 764us/step - loss: 0.2116 - accuracy: 0.7948 - precision: 0.7322 - recall: 0.6351\n",
      "Epoch 231/1000\n",
      "10/10 [==============================] - 0s 771us/step - loss: 0.2115 - accuracy: 0.7948 - precision: 0.7322 - recall: 0.6351\n",
      "Epoch 232/1000\n",
      "10/10 [==============================] - 0s 716us/step - loss: 0.2114 - accuracy: 0.7948 - precision: 0.7322 - recall: 0.6351\n",
      "Epoch 233/1000\n",
      "10/10 [==============================] - 0s 764us/step - loss: 0.2112 - accuracy: 0.7948 - precision: 0.7322 - recall: 0.6351\n",
      "Epoch 234/1000\n",
      "10/10 [==============================] - 0s 731us/step - loss: 0.2112 - accuracy: 0.7948 - precision: 0.7322 - recall: 0.6351\n",
      "Epoch 235/1000\n",
      "10/10 [==============================] - 0s 848us/step - loss: 0.2112 - accuracy: 0.7948 - precision: 0.7322 - recall: 0.6351\n",
      "Epoch 236/1000\n",
      "10/10 [==============================] - 0s 817us/step - loss: 0.2111 - accuracy: 0.7948 - precision: 0.7322 - recall: 0.6351\n",
      "Epoch 237/1000\n",
      "10/10 [==============================] - 0s 759us/step - loss: 0.2110 - accuracy: 0.7948 - precision: 0.7322 - recall: 0.6351\n",
      "Epoch 238/1000\n",
      "10/10 [==============================] - 0s 796us/step - loss: 0.2107 - accuracy: 0.7964 - precision: 0.7363 - recall: 0.6351\n",
      "Epoch 239/1000\n",
      "10/10 [==============================] - 0s 768us/step - loss: 0.2109 - accuracy: 0.7948 - precision: 0.7322 - recall: 0.6351\n",
      "Epoch 240/1000\n",
      "10/10 [==============================] - 0s 740us/step - loss: 0.2108 - accuracy: 0.7964 - precision: 0.7363 - recall: 0.6351\n",
      "Epoch 241/1000\n",
      "10/10 [==============================] - 0s 779us/step - loss: 0.2106 - accuracy: 0.7964 - precision: 0.7363 - recall: 0.6351\n",
      "Epoch 242/1000\n",
      "10/10 [==============================] - 0s 756us/step - loss: 0.2107 - accuracy: 0.7964 - precision: 0.7363 - recall: 0.6351\n",
      "Epoch 243/1000\n",
      "10/10 [==============================] - 0s 798us/step - loss: 0.2107 - accuracy: 0.7964 - precision: 0.7363 - recall: 0.6351\n",
      "Epoch 244/1000\n",
      "10/10 [==============================] - 0s 762us/step - loss: 0.2105 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 245/1000\n",
      "10/10 [==============================] - 0s 767us/step - loss: 0.2105 - accuracy: 0.7964 - precision: 0.7363 - recall: 0.6351\n",
      "Epoch 246/1000\n",
      "10/10 [==============================] - 0s 782us/step - loss: 0.2101 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 247/1000\n",
      "10/10 [==============================] - 0s 723us/step - loss: 0.2102 - accuracy: 0.7964 - precision: 0.7363 - recall: 0.6351\n",
      "Epoch 248/1000\n",
      "10/10 [==============================] - 0s 791us/step - loss: 0.2104 - accuracy: 0.7948 - precision: 0.7348 - recall: 0.6303\n",
      "Epoch 249/1000\n",
      "10/10 [==============================] - 0s 757us/step - loss: 0.2104 - accuracy: 0.7948 - precision: 0.7348 - recall: 0.6303\n",
      "Epoch 250/1000\n",
      "10/10 [==============================] - 0s 765us/step - loss: 0.2101 - accuracy: 0.7964 - precision: 0.7363 - recall: 0.6351\n",
      "Epoch 251/1000\n",
      "10/10 [==============================] - 0s 750us/step - loss: 0.2103 - accuracy: 0.7964 - precision: 0.7363 - recall: 0.6351\n",
      "Epoch 252/1000\n",
      "10/10 [==============================] - 0s 808us/step - loss: 0.2099 - accuracy: 0.7964 - precision: 0.7363 - recall: 0.6351\n",
      "Epoch 253/1000\n",
      "10/10 [==============================] - 0s 729us/step - loss: 0.2101 - accuracy: 0.7964 - precision: 0.7363 - recall: 0.6351\n",
      "Epoch 254/1000\n",
      "10/10 [==============================] - 0s 756us/step - loss: 0.2097 - accuracy: 0.7964 - precision: 0.7363 - recall: 0.6351\n",
      "Epoch 255/1000\n",
      "10/10 [==============================] - 0s 723us/step - loss: 0.2098 - accuracy: 0.7964 - precision: 0.7363 - recall: 0.6351\n",
      "Epoch 256/1000\n",
      "10/10 [==============================] - 0s 738us/step - loss: 0.2098 - accuracy: 0.7964 - precision: 0.7363 - recall: 0.6351\n",
      "Epoch 257/1000\n",
      "10/10 [==============================] - 0s 723us/step - loss: 0.2095 - accuracy: 0.7964 - precision: 0.7363 - recall: 0.6351\n",
      "Epoch 258/1000\n",
      "10/10 [==============================] - 0s 777us/step - loss: 0.2095 - accuracy: 0.7964 - precision: 0.7363 - recall: 0.6351\n",
      "Epoch 259/1000\n",
      "10/10 [==============================] - 0s 752us/step - loss: 0.2096 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 260/1000\n",
      "10/10 [==============================] - 0s 770us/step - loss: 0.2094 - accuracy: 0.7964 - precision: 0.7363 - recall: 0.6351\n",
      "Epoch 261/1000\n",
      "10/10 [==============================] - 0s 720us/step - loss: 0.2092 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 262/1000\n",
      "10/10 [==============================] - 0s 750us/step - loss: 0.2093 - accuracy: 0.7964 - precision: 0.7363 - recall: 0.6351\n",
      "Epoch 263/1000\n",
      "10/10 [==============================] - 0s 728us/step - loss: 0.2092 - accuracy: 0.7964 - precision: 0.7363 - recall: 0.6351\n",
      "Epoch 264/1000\n",
      "10/10 [==============================] - 0s 731us/step - loss: 0.2088 - accuracy: 0.7964 - precision: 0.7363 - recall: 0.6351\n",
      "Epoch 265/1000\n",
      "10/10 [==============================] - 0s 720us/step - loss: 0.2091 - accuracy: 0.7964 - precision: 0.7363 - recall: 0.6351\n",
      "Epoch 266/1000\n",
      "10/10 [==============================] - 0s 745us/step - loss: 0.2090 - accuracy: 0.7964 - precision: 0.7363 - recall: 0.6351\n",
      "Epoch 267/1000\n",
      "10/10 [==============================] - 0s 715us/step - loss: 0.2089 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 268/1000\n",
      "10/10 [==============================] - 0s 761us/step - loss: 0.2089 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 269/1000\n",
      "10/10 [==============================] - 0s 693us/step - loss: 0.2086 - accuracy: 0.7964 - precision: 0.7363 - recall: 0.6351\n",
      "Epoch 270/1000\n",
      "10/10 [==============================] - 0s 703us/step - loss: 0.2089 - accuracy: 0.7964 - precision: 0.7389 - recall: 0.6303\n",
      "Epoch 271/1000\n",
      "10/10 [==============================] - 0s 737us/step - loss: 0.2089 - accuracy: 0.7964 - precision: 0.7363 - recall: 0.6351\n",
      "Epoch 272/1000\n",
      "10/10 [==============================] - 0s 806us/step - loss: 0.2087 - accuracy: 0.7964 - precision: 0.7389 - recall: 0.6303\n",
      "Epoch 273/1000\n",
      "10/10 [==============================] - 0s 750us/step - loss: 0.2086 - accuracy: 0.7964 - precision: 0.7363 - recall: 0.6351\n",
      "Epoch 274/1000\n",
      "10/10 [==============================] - 0s 737us/step - loss: 0.2084 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 275/1000\n",
      "10/10 [==============================] - 0s 753us/step - loss: 0.2084 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 276/1000\n",
      "10/10 [==============================] - 0s 725us/step - loss: 0.2086 - accuracy: 0.7964 - precision: 0.7389 - recall: 0.6303\n",
      "Epoch 277/1000\n",
      "10/10 [==============================] - 0s 709us/step - loss: 0.2082 - accuracy: 0.7964 - precision: 0.7363 - recall: 0.6351\n",
      "Epoch 278/1000\n",
      "10/10 [==============================] - 0s 738us/step - loss: 0.2080 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 279/1000\n",
      "10/10 [==============================] - 0s 750us/step - loss: 0.2084 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 280/1000\n",
      "10/10 [==============================] - 0s 761us/step - loss: 0.2082 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 281/1000\n",
      "10/10 [==============================] - 0s 799us/step - loss: 0.2083 - accuracy: 0.7964 - precision: 0.7389 - recall: 0.6303\n",
      "Epoch 282/1000\n",
      "10/10 [==============================] - 0s 723us/step - loss: 0.2078 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 283/1000\n",
      "10/10 [==============================] - 0s 778us/step - loss: 0.2077 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 284/1000\n",
      "10/10 [==============================] - 0s 799us/step - loss: 0.2079 - accuracy: 0.7964 - precision: 0.7363 - recall: 0.6351\n",
      "Epoch 285/1000\n",
      "10/10 [==============================] - 0s 731us/step - loss: 0.2083 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 286/1000\n",
      "10/10 [==============================] - 0s 761us/step - loss: 0.2077 - accuracy: 0.7964 - precision: 0.7363 - recall: 0.6351\n",
      "Epoch 287/1000\n",
      "10/10 [==============================] - 0s 751us/step - loss: 0.2074 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 288/1000\n",
      "10/10 [==============================] - 0s 777us/step - loss: 0.2077 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 289/1000\n",
      "10/10 [==============================] - 0s 762us/step - loss: 0.2075 - accuracy: 0.7964 - precision: 0.7363 - recall: 0.6351\n",
      "Epoch 290/1000\n",
      "10/10 [==============================] - 0s 746us/step - loss: 0.2075 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 291/1000\n",
      "10/10 [==============================] - 0s 718us/step - loss: 0.2076 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 292/1000\n",
      "10/10 [==============================] - 0s 770us/step - loss: 0.2074 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 293/1000\n",
      "10/10 [==============================] - 0s 789us/step - loss: 0.2075 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 294/1000\n",
      "10/10 [==============================] - 0s 748us/step - loss: 0.2073 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 295/1000\n",
      "10/10 [==============================] - 0s 709us/step - loss: 0.2077 - accuracy: 0.7964 - precision: 0.7389 - recall: 0.6303\n",
      "Epoch 296/1000\n",
      "10/10 [==============================] - 0s 804us/step - loss: 0.2069 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 297/1000\n",
      "10/10 [==============================] - 0s 748us/step - loss: 0.2071 - accuracy: 0.7964 - precision: 0.7363 - recall: 0.6351\n",
      "Epoch 298/1000\n",
      "10/10 [==============================] - 0s 730us/step - loss: 0.2074 - accuracy: 0.7964 - precision: 0.7389 - recall: 0.6303\n",
      "Epoch 299/1000\n",
      "10/10 [==============================] - 0s 755us/step - loss: 0.2074 - accuracy: 0.7964 - precision: 0.7363 - recall: 0.6351\n",
      "Epoch 300/1000\n",
      "10/10 [==============================] - 0s 752us/step - loss: 0.2072 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 301/1000\n",
      "10/10 [==============================] - 0s 793us/step - loss: 0.2070 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 302/1000\n",
      "10/10 [==============================] - 0s 723us/step - loss: 0.2070 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 303/1000\n",
      "10/10 [==============================] - 0s 720us/step - loss: 0.2071 - accuracy: 0.7964 - precision: 0.7363 - recall: 0.6351\n",
      "Epoch 304/1000\n",
      "10/10 [==============================] - 0s 806us/step - loss: 0.2069 - accuracy: 0.7964 - precision: 0.7389 - recall: 0.6303\n",
      "Epoch 305/1000\n",
      "10/10 [==============================] - 0s 751us/step - loss: 0.2070 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 306/1000\n",
      "10/10 [==============================] - 0s 798us/step - loss: 0.2066 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 307/1000\n",
      "10/10 [==============================] - 0s 763us/step - loss: 0.2069 - accuracy: 0.7964 - precision: 0.7389 - recall: 0.6303\n",
      "Epoch 308/1000\n",
      "10/10 [==============================] - 0s 760us/step - loss: 0.2067 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 309/1000\n",
      "10/10 [==============================] - 0s 724us/step - loss: 0.2066 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 310/1000\n",
      "10/10 [==============================] - 0s 753us/step - loss: 0.2066 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 311/1000\n",
      "10/10 [==============================] - 0s 828us/step - loss: 0.2064 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 312/1000\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2065 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 313/1000\n",
      "10/10 [==============================] - 0s 937us/step - loss: 0.2063 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 314/1000\n",
      "10/10 [==============================] - 0s 831us/step - loss: 0.2069 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 315/1000\n",
      "10/10 [==============================] - 0s 801us/step - loss: 0.2064 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 316/1000\n",
      "10/10 [==============================] - 0s 795us/step - loss: 0.2063 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 317/1000\n",
      "10/10 [==============================] - 0s 771us/step - loss: 0.2062 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 318/1000\n",
      "10/10 [==============================] - 0s 763us/step - loss: 0.2063 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 319/1000\n",
      "10/10 [==============================] - 0s 824us/step - loss: 0.2062 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 320/1000\n",
      "10/10 [==============================] - 0s 747us/step - loss: 0.2063 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 321/1000\n",
      "10/10 [==============================] - 0s 747us/step - loss: 0.2059 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 322/1000\n",
      "10/10 [==============================] - 0s 772us/step - loss: 0.2060 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 323/1000\n",
      "10/10 [==============================] - 0s 746us/step - loss: 0.2064 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 324/1000\n",
      "10/10 [==============================] - 0s 773us/step - loss: 0.2061 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 325/1000\n",
      "10/10 [==============================] - 0s 758us/step - loss: 0.2059 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 326/1000\n",
      "10/10 [==============================] - 0s 739us/step - loss: 0.2060 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 327/1000\n",
      "10/10 [==============================] - 0s 750us/step - loss: 0.2060 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 328/1000\n",
      "10/10 [==============================] - 0s 727us/step - loss: 0.2058 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 329/1000\n",
      "10/10 [==============================] - 0s 769us/step - loss: 0.2058 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 330/1000\n",
      "10/10 [==============================] - 0s 702us/step - loss: 0.2058 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 331/1000\n",
      "10/10 [==============================] - 0s 742us/step - loss: 0.2061 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 332/1000\n",
      "10/10 [==============================] - 0s 724us/step - loss: 0.2057 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 333/1000\n",
      "10/10 [==============================] - 0s 716us/step - loss: 0.2056 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 334/1000\n",
      "10/10 [==============================] - 0s 722us/step - loss: 0.2056 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 335/1000\n",
      "10/10 [==============================] - 0s 733us/step - loss: 0.2055 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 336/1000\n",
      "10/10 [==============================] - 0s 688us/step - loss: 0.2057 - accuracy: 0.7964 - precision: 0.7389 - recall: 0.6303\n",
      "Epoch 337/1000\n",
      "10/10 [==============================] - 0s 723us/step - loss: 0.2059 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 338/1000\n",
      "10/10 [==============================] - 0s 687us/step - loss: 0.2057 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 339/1000\n",
      "10/10 [==============================] - 0s 766us/step - loss: 0.2055 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 340/1000\n",
      "10/10 [==============================] - 0s 789us/step - loss: 0.2053 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 341/1000\n",
      "10/10 [==============================] - 0s 751us/step - loss: 0.2051 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 342/1000\n",
      "10/10 [==============================] - 0s 802us/step - loss: 0.2055 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 343/1000\n",
      "10/10 [==============================] - 0s 888us/step - loss: 0.2058 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 344/1000\n",
      "10/10 [==============================] - 0s 801us/step - loss: 0.2054 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 345/1000\n",
      "10/10 [==============================] - 0s 757us/step - loss: 0.2049 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 346/1000\n",
      "10/10 [==============================] - 0s 733us/step - loss: 0.2053 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 347/1000\n",
      "10/10 [==============================] - 0s 712us/step - loss: 0.2050 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 348/1000\n",
      "10/10 [==============================] - 0s 759us/step - loss: 0.2053 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 349/1000\n",
      "10/10 [==============================] - 0s 716us/step - loss: 0.2054 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 350/1000\n",
      "10/10 [==============================] - 0s 820us/step - loss: 0.2051 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 351/1000\n",
      "10/10 [==============================] - 0s 743us/step - loss: 0.2049 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 352/1000\n",
      "10/10 [==============================] - 0s 773us/step - loss: 0.2050 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 353/1000\n",
      "10/10 [==============================] - 0s 723us/step - loss: 0.2053 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 354/1000\n",
      "10/10 [==============================] - 0s 728us/step - loss: 0.2049 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 355/1000\n",
      "10/10 [==============================] - 0s 828us/step - loss: 0.2048 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 356/1000\n",
      "10/10 [==============================] - 0s 707us/step - loss: 0.2047 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 357/1000\n",
      "10/10 [==============================] - 0s 765us/step - loss: 0.2048 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 358/1000\n",
      "10/10 [==============================] - 0s 711us/step - loss: 0.2047 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 359/1000\n",
      "10/10 [==============================] - 0s 711us/step - loss: 0.2048 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 360/1000\n",
      "10/10 [==============================] - 0s 715us/step - loss: 0.2047 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 361/1000\n",
      "10/10 [==============================] - 0s 743us/step - loss: 0.2044 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 362/1000\n",
      "10/10 [==============================] - 0s 771us/step - loss: 0.2048 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 363/1000\n",
      "10/10 [==============================] - 0s 727us/step - loss: 0.2046 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 364/1000\n",
      "10/10 [==============================] - 0s 722us/step - loss: 0.2044 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 365/1000\n",
      "10/10 [==============================] - 0s 736us/step - loss: 0.2048 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 366/1000\n",
      "10/10 [==============================] - 0s 697us/step - loss: 0.2046 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 367/1000\n",
      "10/10 [==============================] - 0s 736us/step - loss: 0.2044 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 368/1000\n",
      "10/10 [==============================] - 0s 748us/step - loss: 0.2047 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 369/1000\n",
      "10/10 [==============================] - 0s 780us/step - loss: 0.2044 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 370/1000\n",
      "10/10 [==============================] - 0s 738us/step - loss: 0.2046 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 371/1000\n",
      "10/10 [==============================] - 0s 729us/step - loss: 0.2046 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 372/1000\n",
      "10/10 [==============================] - 0s 781us/step - loss: 0.2046 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 373/1000\n",
      "10/10 [==============================] - 0s 753us/step - loss: 0.2043 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 374/1000\n",
      "10/10 [==============================] - 0s 747us/step - loss: 0.2043 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 375/1000\n",
      "10/10 [==============================] - 0s 763us/step - loss: 0.2042 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 376/1000\n",
      "10/10 [==============================] - 0s 811us/step - loss: 0.2045 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 377/1000\n",
      "10/10 [==============================] - 0s 789us/step - loss: 0.2044 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 378/1000\n",
      "10/10 [==============================] - 0s 750us/step - loss: 0.2039 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 379/1000\n",
      "10/10 [==============================] - 0s 829us/step - loss: 0.2043 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 380/1000\n",
      "10/10 [==============================] - 0s 746us/step - loss: 0.2044 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 381/1000\n",
      "10/10 [==============================] - 0s 770us/step - loss: 0.2042 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 382/1000\n",
      "10/10 [==============================] - 0s 795us/step - loss: 0.2041 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 383/1000\n",
      "10/10 [==============================] - 0s 798us/step - loss: 0.2041 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 384/1000\n",
      "10/10 [==============================] - 0s 762us/step - loss: 0.2041 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 385/1000\n",
      "10/10 [==============================] - 0s 878us/step - loss: 0.2040 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 386/1000\n",
      "10/10 [==============================] - 0s 739us/step - loss: 0.2040 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 387/1000\n",
      "10/10 [==============================] - 0s 741us/step - loss: 0.2039 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 388/1000\n",
      "10/10 [==============================] - 0s 696us/step - loss: 0.2041 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 389/1000\n",
      "10/10 [==============================] - 0s 682us/step - loss: 0.2042 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 390/1000\n",
      "10/10 [==============================] - 0s 784us/step - loss: 0.2040 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 391/1000\n",
      "10/10 [==============================] - 0s 694us/step - loss: 0.2039 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 392/1000\n",
      "10/10 [==============================] - 0s 747us/step - loss: 0.2041 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 393/1000\n",
      "10/10 [==============================] - 0s 687us/step - loss: 0.2039 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 394/1000\n",
      "10/10 [==============================] - 0s 768us/step - loss: 0.2038 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 395/1000\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.2038 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 396/1000\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2037 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 397/1000\n",
      "10/10 [==============================] - 0s 713us/step - loss: 0.2037 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 398/1000\n",
      "10/10 [==============================] - 0s 687us/step - loss: 0.2037 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 399/1000\n",
      "10/10 [==============================] - 0s 697us/step - loss: 0.2036 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 400/1000\n",
      "10/10 [==============================] - 0s 750us/step - loss: 0.2040 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 401/1000\n",
      "10/10 [==============================] - 0s 757us/step - loss: 0.2034 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 402/1000\n",
      "10/10 [==============================] - 0s 733us/step - loss: 0.2035 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 403/1000\n",
      "10/10 [==============================] - 0s 757us/step - loss: 0.2043 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 404/1000\n",
      "10/10 [==============================] - 0s 773us/step - loss: 0.2037 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 405/1000\n",
      "10/10 [==============================] - 0s 743us/step - loss: 0.2034 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 406/1000\n",
      "10/10 [==============================] - 0s 812us/step - loss: 0.2033 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 407/1000\n",
      "10/10 [==============================] - 0s 725us/step - loss: 0.2034 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 408/1000\n",
      "10/10 [==============================] - 0s 784us/step - loss: 0.2038 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 409/1000\n",
      "10/10 [==============================] - 0s 757us/step - loss: 0.2034 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 410/1000\n",
      "10/10 [==============================] - 0s 717us/step - loss: 0.2034 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 411/1000\n",
      "10/10 [==============================] - 0s 756us/step - loss: 0.2038 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 412/1000\n",
      "10/10 [==============================] - 0s 783us/step - loss: 0.2034 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 413/1000\n",
      "10/10 [==============================] - 0s 832us/step - loss: 0.2033 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 414/1000\n",
      "10/10 [==============================] - 0s 802us/step - loss: 0.2037 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 415/1000\n",
      "10/10 [==============================] - 0s 797us/step - loss: 0.2033 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 416/1000\n",
      "10/10 [==============================] - 0s 875us/step - loss: 0.2039 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 417/1000\n",
      "10/10 [==============================] - 0s 748us/step - loss: 0.2033 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 418/1000\n",
      "10/10 [==============================] - 0s 801us/step - loss: 0.2031 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 419/1000\n",
      "10/10 [==============================] - 0s 722us/step - loss: 0.2036 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 420/1000\n",
      "10/10 [==============================] - 0s 780us/step - loss: 0.2033 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 421/1000\n",
      "10/10 [==============================] - 0s 753us/step - loss: 0.2032 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 422/1000\n",
      "10/10 [==============================] - 0s 764us/step - loss: 0.2035 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 423/1000\n",
      "10/10 [==============================] - 0s 736us/step - loss: 0.2033 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 424/1000\n",
      "10/10 [==============================] - 0s 717us/step - loss: 0.2033 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 425/1000\n",
      "10/10 [==============================] - 0s 832us/step - loss: 0.2034 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 426/1000\n",
      "10/10 [==============================] - 0s 769us/step - loss: 0.2032 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 427/1000\n",
      "10/10 [==============================] - 0s 764us/step - loss: 0.2032 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 428/1000\n",
      "10/10 [==============================] - 0s 730us/step - loss: 0.2030 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 429/1000\n",
      "10/10 [==============================] - 0s 723us/step - loss: 0.2036 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 430/1000\n",
      "10/10 [==============================] - 0s 820us/step - loss: 0.2029 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 431/1000\n",
      "10/10 [==============================] - 0s 726us/step - loss: 0.2033 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 432/1000\n",
      "10/10 [==============================] - 0s 772us/step - loss: 0.2035 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 433/1000\n",
      "10/10 [==============================] - 0s 725us/step - loss: 0.2033 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 434/1000\n",
      "10/10 [==============================] - 0s 776us/step - loss: 0.2030 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 435/1000\n",
      "10/10 [==============================] - 0s 740us/step - loss: 0.2030 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 436/1000\n",
      "10/10 [==============================] - 0s 760us/step - loss: 0.2031 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 437/1000\n",
      "10/10 [==============================] - 0s 741us/step - loss: 0.2034 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 438/1000\n",
      "10/10 [==============================] - 0s 696us/step - loss: 0.2030 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 439/1000\n",
      "10/10 [==============================] - 0s 704us/step - loss: 0.2029 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 440/1000\n",
      "10/10 [==============================] - 0s 687us/step - loss: 0.2030 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 441/1000\n",
      "10/10 [==============================] - 0s 710us/step - loss: 0.2030 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 442/1000\n",
      "10/10 [==============================] - 0s 762us/step - loss: 0.2030 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 443/1000\n",
      "10/10 [==============================] - 0s 746us/step - loss: 0.2032 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 444/1000\n",
      "10/10 [==============================] - 0s 772us/step - loss: 0.2028 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 445/1000\n",
      "10/10 [==============================] - 0s 707us/step - loss: 0.2031 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 446/1000\n",
      "10/10 [==============================] - 0s 762us/step - loss: 0.2029 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 447/1000\n",
      "10/10 [==============================] - 0s 749us/step - loss: 0.2027 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 448/1000\n",
      "10/10 [==============================] - 0s 779us/step - loss: 0.2032 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 449/1000\n",
      "10/10 [==============================] - 0s 723us/step - loss: 0.2030 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 450/1000\n",
      "10/10 [==============================] - 0s 735us/step - loss: 0.2029 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 451/1000\n",
      "10/10 [==============================] - 0s 766us/step - loss: 0.2030 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 452/1000\n",
      "10/10 [==============================] - 0s 712us/step - loss: 0.2031 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 453/1000\n",
      "10/10 [==============================] - 0s 862us/step - loss: 0.2028 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 454/1000\n",
      "10/10 [==============================] - 0s 746us/step - loss: 0.2029 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 455/1000\n",
      "10/10 [==============================] - 0s 751us/step - loss: 0.2031 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 456/1000\n",
      "10/10 [==============================] - 0s 726us/step - loss: 0.2028 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 457/1000\n",
      "10/10 [==============================] - 0s 745us/step - loss: 0.2030 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 458/1000\n",
      "10/10 [==============================] - 0s 748us/step - loss: 0.2026 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 459/1000\n",
      "10/10 [==============================] - 0s 777us/step - loss: 0.2028 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 460/1000\n",
      "10/10 [==============================] - 0s 742us/step - loss: 0.2027 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 461/1000\n",
      "10/10 [==============================] - 0s 782us/step - loss: 0.2028 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 462/1000\n",
      "10/10 [==============================] - 0s 797us/step - loss: 0.2027 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 463/1000\n",
      "10/10 [==============================] - 0s 728us/step - loss: 0.2032 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 464/1000\n",
      "10/10 [==============================] - 0s 767us/step - loss: 0.2028 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 465/1000\n",
      "10/10 [==============================] - 0s 775us/step - loss: 0.2026 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 466/1000\n",
      "10/10 [==============================] - 0s 745us/step - loss: 0.2028 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 467/1000\n",
      "10/10 [==============================] - 0s 723us/step - loss: 0.2027 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 468/1000\n",
      "10/10 [==============================] - 0s 728us/step - loss: 0.2026 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 469/1000\n",
      "10/10 [==============================] - 0s 726us/step - loss: 0.2031 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 470/1000\n",
      "10/10 [==============================] - 0s 751us/step - loss: 0.2026 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 471/1000\n",
      "10/10 [==============================] - 0s 706us/step - loss: 0.2026 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 472/1000\n",
      "10/10 [==============================] - 0s 758us/step - loss: 0.2026 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 473/1000\n",
      "10/10 [==============================] - 0s 701us/step - loss: 0.2027 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 474/1000\n",
      "10/10 [==============================] - 0s 704us/step - loss: 0.2028 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 475/1000\n",
      "10/10 [==============================] - 0s 700us/step - loss: 0.2026 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 476/1000\n",
      "10/10 [==============================] - 0s 739us/step - loss: 0.2029 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 477/1000\n",
      "10/10 [==============================] - 0s 690us/step - loss: 0.2027 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 478/1000\n",
      "10/10 [==============================] - 0s 707us/step - loss: 0.2025 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 479/1000\n",
      "10/10 [==============================] - 0s 724us/step - loss: 0.2026 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 480/1000\n",
      "10/10 [==============================] - 0s 747us/step - loss: 0.2027 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 481/1000\n",
      "10/10 [==============================] - 0s 798us/step - loss: 0.2026 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 482/1000\n",
      "10/10 [==============================] - 0s 761us/step - loss: 0.2024 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 483/1000\n",
      "10/10 [==============================] - 0s 743us/step - loss: 0.2026 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 484/1000\n",
      "10/10 [==============================] - 0s 764us/step - loss: 0.2027 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 485/1000\n",
      "10/10 [==============================] - 0s 726us/step - loss: 0.2026 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 486/1000\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.2025 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 487/1000\n",
      "10/10 [==============================] - 0s 755us/step - loss: 0.2025 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 488/1000\n",
      "10/10 [==============================] - 0s 843us/step - loss: 0.2025 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 489/1000\n",
      "10/10 [==============================] - 0s 741us/step - loss: 0.2027 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 490/1000\n",
      "10/10 [==============================] - 0s 808us/step - loss: 0.2024 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 491/1000\n",
      "10/10 [==============================] - 0s 758us/step - loss: 0.2030 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 492/1000\n",
      "10/10 [==============================] - 0s 702us/step - loss: 0.2024 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 493/1000\n",
      "10/10 [==============================] - 0s 695us/step - loss: 0.2023 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 494/1000\n",
      "10/10 [==============================] - 0s 791us/step - loss: 0.2024 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 495/1000\n",
      "10/10 [==============================] - 0s 701us/step - loss: 0.2025 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 496/1000\n",
      "10/10 [==============================] - 0s 852us/step - loss: 0.2026 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 497/1000\n",
      "10/10 [==============================] - 0s 713us/step - loss: 0.2024 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 498/1000\n",
      "10/10 [==============================] - 0s 785us/step - loss: 0.2025 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 499/1000\n",
      "10/10 [==============================] - 0s 830us/step - loss: 0.2025 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 500/1000\n",
      "10/10 [==============================] - 0s 816us/step - loss: 0.2023 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 501/1000\n",
      "10/10 [==============================] - 0s 863us/step - loss: 0.2024 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 502/1000\n",
      "10/10 [==============================] - 0s 708us/step - loss: 0.2024 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 503/1000\n",
      "10/10 [==============================] - 0s 780us/step - loss: 0.2026 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 504/1000\n",
      "10/10 [==============================] - 0s 720us/step - loss: 0.2024 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 505/1000\n",
      "10/10 [==============================] - 0s 701us/step - loss: 0.2025 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 506/1000\n",
      "10/10 [==============================] - 0s 733us/step - loss: 0.2023 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 507/1000\n",
      "10/10 [==============================] - 0s 784us/step - loss: 0.2024 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 508/1000\n",
      "10/10 [==============================] - 0s 753us/step - loss: 0.2026 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 509/1000\n",
      "10/10 [==============================] - 0s 759us/step - loss: 0.2023 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 510/1000\n",
      "10/10 [==============================] - 0s 743us/step - loss: 0.2023 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 511/1000\n",
      "10/10 [==============================] - 0s 743us/step - loss: 0.2023 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 512/1000\n",
      "10/10 [==============================] - 0s 729us/step - loss: 0.2028 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 513/1000\n",
      "10/10 [==============================] - 0s 748us/step - loss: 0.2024 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 514/1000\n",
      "10/10 [==============================] - 0s 738us/step - loss: 0.2023 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 515/1000\n",
      "10/10 [==============================] - 0s 751us/step - loss: 0.2023 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 516/1000\n",
      "10/10 [==============================] - 0s 759us/step - loss: 0.2023 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 517/1000\n",
      "10/10 [==============================] - 0s 775us/step - loss: 0.2024 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 518/1000\n",
      "10/10 [==============================] - 0s 753us/step - loss: 0.2024 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 519/1000\n",
      "10/10 [==============================] - 0s 747us/step - loss: 0.2023 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 520/1000\n",
      "10/10 [==============================] - 0s 704us/step - loss: 0.2023 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 521/1000\n",
      "10/10 [==============================] - 0s 772us/step - loss: 0.2023 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 522/1000\n",
      "10/10 [==============================] - 0s 701us/step - loss: 0.2023 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 523/1000\n",
      "10/10 [==============================] - 0s 774us/step - loss: 0.2023 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 524/1000\n",
      "10/10 [==============================] - 0s 759us/step - loss: 0.2023 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 525/1000\n",
      "10/10 [==============================] - 0s 760us/step - loss: 0.2024 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 526/1000\n",
      "10/10 [==============================] - 0s 769us/step - loss: 0.2022 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 527/1000\n",
      "10/10 [==============================] - 0s 709us/step - loss: 0.2024 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 528/1000\n",
      "10/10 [==============================] - 0s 794us/step - loss: 0.2023 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 529/1000\n",
      "10/10 [==============================] - 0s 731us/step - loss: 0.2023 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 530/1000\n",
      "10/10 [==============================] - 0s 712us/step - loss: 0.2022 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 531/1000\n",
      "10/10 [==============================] - 0s 787us/step - loss: 0.2023 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 532/1000\n",
      "10/10 [==============================] - 0s 757us/step - loss: 0.2022 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 533/1000\n",
      "10/10 [==============================] - 0s 829us/step - loss: 0.2023 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 534/1000\n",
      "10/10 [==============================] - 0s 701us/step - loss: 0.2022 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 535/1000\n",
      "10/10 [==============================] - 0s 754us/step - loss: 0.2022 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 536/1000\n",
      "10/10 [==============================] - 0s 682us/step - loss: 0.2024 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 537/1000\n",
      "10/10 [==============================] - 0s 717us/step - loss: 0.2022 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 538/1000\n",
      "10/10 [==============================] - 0s 744us/step - loss: 0.2022 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 539/1000\n",
      "10/10 [==============================] - 0s 713us/step - loss: 0.2022 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 540/1000\n",
      "10/10 [==============================] - 0s 681us/step - loss: 0.2022 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 541/1000\n",
      "10/10 [==============================] - 0s 693us/step - loss: 0.2024 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 542/1000\n",
      "10/10 [==============================] - 0s 747us/step - loss: 0.2022 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 543/1000\n",
      "10/10 [==============================] - 0s 714us/step - loss: 0.2022 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 544/1000\n",
      "10/10 [==============================] - 0s 749us/step - loss: 0.2022 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 545/1000\n",
      "10/10 [==============================] - 0s 717us/step - loss: 0.2023 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 546/1000\n",
      "10/10 [==============================] - 0s 727us/step - loss: 0.2022 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 547/1000\n",
      "10/10 [==============================] - 0s 723us/step - loss: 0.2023 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 548/1000\n",
      "10/10 [==============================] - 0s 709us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 549/1000\n",
      "10/10 [==============================] - 0s 713us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 550/1000\n",
      "10/10 [==============================] - 0s 701us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 551/1000\n",
      "10/10 [==============================] - 0s 778us/step - loss: 0.2022 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 552/1000\n",
      "10/10 [==============================] - 0s 745us/step - loss: 0.2022 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 553/1000\n",
      "10/10 [==============================] - 0s 752us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 554/1000\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2193 - accuracy: 0.7812 - precision: 0.7143 - recall: 0.652 - 0s 755us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 555/1000\n",
      "10/10 [==============================] - 0s 728us/step - loss: 0.2022 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 556/1000\n",
      "10/10 [==============================] - 0s 716us/step - loss: 0.2022 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 557/1000\n",
      "10/10 [==============================] - 0s 815us/step - loss: 0.2022 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 558/1000\n",
      "10/10 [==============================] - 0s 931us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 559/1000\n",
      "10/10 [==============================] - 0s 797us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 560/1000\n",
      "10/10 [==============================] - 0s 761us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 561/1000\n",
      "10/10 [==============================] - 0s 775us/step - loss: 0.2024 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 562/1000\n",
      "10/10 [==============================] - 0s 721us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 563/1000\n",
      "10/10 [==============================] - 0s 837us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 564/1000\n",
      "10/10 [==============================] - 0s 785us/step - loss: 0.2022 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 565/1000\n",
      "10/10 [==============================] - 0s 697us/step - loss: 0.2023 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 566/1000\n",
      "10/10 [==============================] - 0s 750us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 567/1000\n",
      "10/10 [==============================] - 0s 749us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 568/1000\n",
      "10/10 [==============================] - 0s 687us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 569/1000\n",
      "10/10 [==============================] - 0s 757us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 570/1000\n",
      "10/10 [==============================] - 0s 785us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 571/1000\n",
      "10/10 [==============================] - 0s 717us/step - loss: 0.2022 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 572/1000\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1875 - accuracy: 0.8125 - precision: 0.8750 - recall: 0.583 - 0s 709us/step - loss: 0.2022 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 573/1000\n",
      "10/10 [==============================] - 0s 742us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 574/1000\n",
      "10/10 [==============================] - 0s 735us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 575/1000\n",
      "10/10 [==============================] - 0s 790us/step - loss: 0.2022 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 576/1000\n",
      "10/10 [==============================] - 0s 767us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 577/1000\n",
      "10/10 [==============================] - 0s 746us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 578/1000\n",
      "10/10 [==============================] - 0s 730us/step - loss: 0.2022 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 579/1000\n",
      "10/10 [==============================] - 0s 734us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 580/1000\n",
      "10/10 [==============================] - 0s 691us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 581/1000\n",
      "10/10 [==============================] - 0s 765us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 582/1000\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2022 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 583/1000\n",
      "10/10 [==============================] - 0s 937us/step - loss: 0.2022 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 584/1000\n",
      "10/10 [==============================] - 0s 722us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 585/1000\n",
      "10/10 [==============================] - 0s 721us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 586/1000\n",
      "10/10 [==============================] - 0s 731us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 587/1000\n",
      "10/10 [==============================] - 0s 719us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 588/1000\n",
      "10/10 [==============================] - 0s 752us/step - loss: 0.2023 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 589/1000\n",
      "10/10 [==============================] - 0s 707us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 590/1000\n",
      "10/10 [==============================] - 0s 728us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 591/1000\n",
      "10/10 [==============================] - 0s 721us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 592/1000\n",
      "10/10 [==============================] - 0s 718us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 593/1000\n",
      "10/10 [==============================] - 0s 725us/step - loss: 0.2022 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 594/1000\n",
      "10/10 [==============================] - 0s 747us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 595/1000\n",
      "10/10 [==============================] - 0s 766us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 596/1000\n",
      "10/10 [==============================] - 0s 723us/step - loss: 0.2022 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 597/1000\n",
      "10/10 [==============================] - 0s 683us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 598/1000\n",
      "10/10 [==============================] - 0s 726us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 599/1000\n",
      "10/10 [==============================] - 0s 722us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 600/1000\n",
      "10/10 [==============================] - 0s 705us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 601/1000\n",
      "10/10 [==============================] - 0s 743us/step - loss: 0.2024 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 602/1000\n",
      "10/10 [==============================] - 0s 710us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 603/1000\n",
      "10/10 [==============================] - 0s 732us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 604/1000\n",
      "10/10 [==============================] - 0s 686us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 605/1000\n",
      "10/10 [==============================] - 0s 728us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 606/1000\n",
      "10/10 [==============================] - 0s 702us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 607/1000\n",
      "10/10 [==============================] - 0s 697us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 608/1000\n",
      "10/10 [==============================] - 0s 755us/step - loss: 0.2022 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 609/1000\n",
      "10/10 [==============================] - 0s 717us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 610/1000\n",
      "10/10 [==============================] - 0s 756us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 611/1000\n",
      "10/10 [==============================] - 0s 727us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 612/1000\n",
      "10/10 [==============================] - 0s 774us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 613/1000\n",
      "10/10 [==============================] - 0s 727us/step - loss: 0.2023 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 614/1000\n",
      "10/10 [==============================] - 0s 729us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 615/1000\n",
      "10/10 [==============================] - 0s 737us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 616/1000\n",
      "10/10 [==============================] - 0s 745us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 617/1000\n",
      "10/10 [==============================] - 0s 686us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 618/1000\n",
      "10/10 [==============================] - 0s 771us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 619/1000\n",
      "10/10 [==============================] - 0s 709us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 620/1000\n",
      "10/10 [==============================] - 0s 762us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 621/1000\n",
      "10/10 [==============================] - 0s 727us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 622/1000\n",
      "10/10 [==============================] - 0s 743us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 623/1000\n",
      "10/10 [==============================] - 0s 772us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 624/1000\n",
      "10/10 [==============================] - 0s 809us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 625/1000\n",
      "10/10 [==============================] - 0s 728us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 626/1000\n",
      "10/10 [==============================] - 0s 978us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 627/1000\n",
      "10/10 [==============================] - 0s 804us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 628/1000\n",
      "10/10 [==============================] - 0s 741us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 629/1000\n",
      "10/10 [==============================] - 0s 795us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 630/1000\n",
      "10/10 [==============================] - 0s 652us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 631/1000\n",
      "10/10 [==============================] - 0s 762us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 632/1000\n",
      "10/10 [==============================] - 0s 708us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 633/1000\n",
      "10/10 [==============================] - 0s 813us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 634/1000\n",
      "10/10 [==============================] - 0s 697us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 635/1000\n",
      "10/10 [==============================] - 0s 733us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 636/1000\n",
      "10/10 [==============================] - 0s 759us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 637/1000\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1875 - accuracy: 0.8125 - precision: 0.7500 - recall: 0.681 - 0s 692us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 638/1000\n",
      "10/10 [==============================] - 0s 775us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 639/1000\n",
      "10/10 [==============================] - 0s 693us/step - loss: 0.2027 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 640/1000\n",
      "10/10 [==============================] - 0s 787us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 641/1000\n",
      "10/10 [==============================] - 0s 749us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 642/1000\n",
      "10/10 [==============================] - 0s 783us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 643/1000\n",
      "10/10 [==============================] - 0s 762us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 644/1000\n",
      "10/10 [==============================] - 0s 767us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 645/1000\n",
      "10/10 [==============================] - 0s 695us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 646/1000\n",
      "10/10 [==============================] - 0s 777us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 647/1000\n",
      "10/10 [==============================] - 0s 749us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 648/1000\n",
      "10/10 [==============================] - 0s 742us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 649/1000\n",
      "10/10 [==============================] - 0s 756us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 650/1000\n",
      "10/10 [==============================] - 0s 721us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 651/1000\n",
      "10/10 [==============================] - 0s 718us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 652/1000\n",
      "10/10 [==============================] - 0s 721us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 653/1000\n",
      "10/10 [==============================] - 0s 763us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 654/1000\n",
      "10/10 [==============================] - 0s 756us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 655/1000\n",
      "10/10 [==============================] - 0s 737us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 656/1000\n",
      "10/10 [==============================] - 0s 766us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 657/1000\n",
      "10/10 [==============================] - 0s 755us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 658/1000\n",
      "10/10 [==============================] - 0s 696us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 659/1000\n",
      "10/10 [==============================] - 0s 765us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 660/1000\n",
      "10/10 [==============================] - 0s 739us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 661/1000\n",
      "10/10 [==============================] - 0s 687us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 662/1000\n",
      "10/10 [==============================] - 0s 705us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 663/1000\n",
      "10/10 [==============================] - 0s 743us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 664/1000\n",
      "10/10 [==============================] - 0s 705us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 665/1000\n",
      "10/10 [==============================] - 0s 753us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 666/1000\n",
      "10/10 [==============================] - 0s 749us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 667/1000\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 668/1000\n",
      "10/10 [==============================] - 0s 923us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 669/1000\n",
      "10/10 [==============================] - 0s 766us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 670/1000\n",
      "10/10 [==============================] - 0s 771us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 671/1000\n",
      "10/10 [==============================] - 0s 704us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 672/1000\n",
      "10/10 [==============================] - 0s 754us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 673/1000\n",
      "10/10 [==============================] - 0s 709us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 674/1000\n",
      "10/10 [==============================] - 0s 741us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 675/1000\n",
      "10/10 [==============================] - 0s 740us/step - loss: 0.2029 - accuracy: 0.7964 - precision: 0.7389 - recall: 0.6303\n",
      "Epoch 676/1000\n",
      "10/10 [==============================] - 0s 731us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 677/1000\n",
      "10/10 [==============================] - 0s 760us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 678/1000\n",
      "10/10 [==============================] - 0s 746us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 679/1000\n",
      "10/10 [==============================] - 0s 774us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 680/1000\n",
      "10/10 [==============================] - 0s 710us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 681/1000\n",
      "10/10 [==============================] - 0s 852us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 682/1000\n",
      "10/10 [==============================] - 0s 689us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 683/1000\n",
      "10/10 [==============================] - 0s 688us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 684/1000\n",
      "10/10 [==============================] - 0s 727us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 685/1000\n",
      "10/10 [==============================] - 0s 741us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 686/1000\n",
      "10/10 [==============================] - 0s 736us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 687/1000\n",
      "10/10 [==============================] - 0s 753us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 688/1000\n",
      "10/10 [==============================] - 0s 746us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 689/1000\n",
      "10/10 [==============================] - 0s 722us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 690/1000\n",
      "10/10 [==============================] - 0s 721us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 691/1000\n",
      "10/10 [==============================] - 0s 712us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 692/1000\n",
      "10/10 [==============================] - 0s 760us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 693/1000\n",
      "10/10 [==============================] - 0s 756us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 694/1000\n",
      "10/10 [==============================] - 0s 761us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 695/1000\n",
      "10/10 [==============================] - 0s 710us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 696/1000\n",
      "10/10 [==============================] - 0s 715us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 697/1000\n",
      "10/10 [==============================] - 0s 707us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 698/1000\n",
      "10/10 [==============================] - 0s 729us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 699/1000\n",
      "10/10 [==============================] - 0s 688us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 700/1000\n",
      "10/10 [==============================] - 0s 761us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 701/1000\n",
      "10/10 [==============================] - 0s 703us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 702/1000\n",
      "10/10 [==============================] - 0s 883us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 703/1000\n",
      "10/10 [==============================] - 0s 748us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 704/1000\n",
      "10/10 [==============================] - 0s 834us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 705/1000\n",
      "10/10 [==============================] - 0s 742us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 706/1000\n",
      "10/10 [==============================] - 0s 758us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 707/1000\n",
      "10/10 [==============================] - 0s 784us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 708/1000\n",
      "10/10 [==============================] - 0s 827us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 709/1000\n",
      "10/10 [==============================] - 0s 817us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 710/1000\n",
      "10/10 [==============================] - 0s 737us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 711/1000\n",
      "10/10 [==============================] - 0s 794us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 712/1000\n",
      "10/10 [==============================] - 0s 744us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 713/1000\n",
      "10/10 [==============================] - 0s 811us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 714/1000\n",
      "10/10 [==============================] - 0s 709us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 715/1000\n",
      "10/10 [==============================] - 0s 799us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 716/1000\n",
      "10/10 [==============================] - 0s 783us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 717/1000\n",
      "10/10 [==============================] - 0s 695us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 718/1000\n",
      "10/10 [==============================] - 0s 714us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 719/1000\n",
      "10/10 [==============================] - 0s 772us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 720/1000\n",
      "10/10 [==============================] - 0s 718us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 721/1000\n",
      "10/10 [==============================] - 0s 756us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 722/1000\n",
      "10/10 [==============================] - 0s 733us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 723/1000\n",
      "10/10 [==============================] - 0s 738us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 724/1000\n",
      "10/10 [==============================] - 0s 756us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 725/1000\n",
      "10/10 [==============================] - 0s 730us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 726/1000\n",
      "10/10 [==============================] - 0s 750us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 727/1000\n",
      "10/10 [==============================] - 0s 800us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 728/1000\n",
      "10/10 [==============================] - 0s 740us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 729/1000\n",
      "10/10 [==============================] - 0s 715us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 730/1000\n",
      "10/10 [==============================] - 0s 737us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 731/1000\n",
      "10/10 [==============================] - 0s 729us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 732/1000\n",
      "10/10 [==============================] - 0s 745us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 733/1000\n",
      "10/10 [==============================] - 0s 716us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 734/1000\n",
      "10/10 [==============================] - 0s 731us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 735/1000\n",
      "10/10 [==============================] - 0s 734us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 736/1000\n",
      "10/10 [==============================] - 0s 810us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 737/1000\n",
      "10/10 [==============================] - 0s 780us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 738/1000\n",
      "10/10 [==============================] - 0s 820us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 739/1000\n",
      "10/10 [==============================] - 0s 791us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 740/1000\n",
      "10/10 [==============================] - 0s 731us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 741/1000\n",
      "10/10 [==============================] - 0s 806us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 742/1000\n",
      "10/10 [==============================] - 0s 764us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 743/1000\n",
      "10/10 [==============================] - 0s 715us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 744/1000\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 745/1000\n",
      "10/10 [==============================] - 0s 982us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 746/1000\n",
      "10/10 [==============================] - 0s 753us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 747/1000\n",
      "10/10 [==============================] - 0s 888us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 748/1000\n",
      "10/10 [==============================] - 0s 789us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 749/1000\n",
      "10/10 [==============================] - 0s 849us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 750/1000\n",
      "10/10 [==============================] - 0s 745us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 751/1000\n",
      "10/10 [==============================] - 0s 721us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 752/1000\n",
      "10/10 [==============================] - 0s 711us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 753/1000\n",
      "10/10 [==============================] - 0s 677us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 754/1000\n",
      "10/10 [==============================] - 0s 825us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 755/1000\n",
      "10/10 [==============================] - 0s 685us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 756/1000\n",
      "10/10 [==============================] - 0s 757us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 757/1000\n",
      "10/10 [==============================] - 0s 703us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 758/1000\n",
      "10/10 [==============================] - 0s 708us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 759/1000\n",
      "10/10 [==============================] - 0s 745us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 760/1000\n",
      "10/10 [==============================] - 0s 740us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 761/1000\n",
      "10/10 [==============================] - 0s 711us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 762/1000\n",
      "10/10 [==============================] - 0s 690us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 763/1000\n",
      "10/10 [==============================] - 0s 696us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 764/1000\n",
      "10/10 [==============================] - 0s 759us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 765/1000\n",
      "10/10 [==============================] - 0s 706us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 766/1000\n",
      "10/10 [==============================] - 0s 714us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 767/1000\n",
      "10/10 [==============================] - 0s 704us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 768/1000\n",
      "10/10 [==============================] - 0s 712us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 769/1000\n",
      "10/10 [==============================] - 0s 683us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 770/1000\n",
      "10/10 [==============================] - 0s 699us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 771/1000\n",
      "10/10 [==============================] - 0s 739us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 772/1000\n",
      "10/10 [==============================] - 0s 727us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 773/1000\n",
      "10/10 [==============================] - 0s 741us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 774/1000\n",
      "10/10 [==============================] - 0s 723us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 775/1000\n",
      "10/10 [==============================] - 0s 713us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 776/1000\n",
      "10/10 [==============================] - 0s 764us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 777/1000\n",
      "10/10 [==============================] - 0s 751us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 778/1000\n",
      "10/10 [==============================] - 0s 740us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 779/1000\n",
      "10/10 [==============================] - 0s 784us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 780/1000\n",
      "10/10 [==============================] - 0s 775us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 781/1000\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2188 - accuracy: 0.7812 - precision: 0.7692 - recall: 0.476 - 0s 725us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 782/1000\n",
      "10/10 [==============================] - 0s 800us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 783/1000\n",
      "10/10 [==============================] - 0s 717us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 784/1000\n",
      "10/10 [==============================] - 0s 774us/step - loss: 0.2021 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 785/1000\n",
      "10/10 [==============================] - 0s 711us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 786/1000\n",
      "10/10 [==============================] - 0s 833us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 787/1000\n",
      "10/10 [==============================] - 0s 763us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 788/1000\n",
      "10/10 [==============================] - 0s 769us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 789/1000\n",
      "10/10 [==============================] - 0s 746us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 790/1000\n",
      "10/10 [==============================] - 0s 768us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 791/1000\n",
      "10/10 [==============================] - 0s 717us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 792/1000\n",
      "10/10 [==============================] - 0s 724us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 793/1000\n",
      "10/10 [==============================] - 0s 759us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 794/1000\n",
      "10/10 [==============================] - 0s 723us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 795/1000\n",
      "10/10 [==============================] - 0s 740us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 796/1000\n",
      "10/10 [==============================] - 0s 735us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 797/1000\n",
      "10/10 [==============================] - 0s 750us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 798/1000\n",
      "10/10 [==============================] - 0s 749us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 799/1000\n",
      "10/10 [==============================] - 0s 721us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 800/1000\n",
      "10/10 [==============================] - 0s 726us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 801/1000\n",
      "10/10 [==============================] - 0s 810us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 802/1000\n",
      "10/10 [==============================] - 0s 737us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 803/1000\n",
      "10/10 [==============================] - 0s 711us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 804/1000\n",
      "10/10 [==============================] - 0s 788us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 805/1000\n",
      "10/10 [==============================] - 0s 774us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 806/1000\n",
      "10/10 [==============================] - 0s 719us/step - loss: 0.2022 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 807/1000\n",
      "10/10 [==============================] - 0s 764us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 808/1000\n",
      "10/10 [==============================] - 0s 784us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 809/1000\n",
      "10/10 [==============================] - 0s 736us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 810/1000\n",
      "10/10 [==============================] - 0s 769us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 811/1000\n",
      "10/10 [==============================] - 0s 749us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 812/1000\n",
      "10/10 [==============================] - 0s 794us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 813/1000\n",
      "10/10 [==============================] - 0s 719us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 814/1000\n",
      "10/10 [==============================] - 0s 751us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 815/1000\n",
      "10/10 [==============================] - 0s 675us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 816/1000\n",
      "10/10 [==============================] - 0s 712us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 817/1000\n",
      "10/10 [==============================] - 0s 759us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 818/1000\n",
      "10/10 [==============================] - 0s 730us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 819/1000\n",
      "10/10 [==============================] - 0s 709us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 820/1000\n",
      "10/10 [==============================] - 0s 722us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 821/1000\n",
      "10/10 [==============================] - 0s 752us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 822/1000\n",
      "10/10 [==============================] - 0s 756us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 823/1000\n",
      "10/10 [==============================] - 0s 744us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 824/1000\n",
      "10/10 [==============================] - 0s 701us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 825/1000\n",
      "10/10 [==============================] - 0s 722us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 826/1000\n",
      "10/10 [==============================] - 0s 683us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 827/1000\n",
      "10/10 [==============================] - 0s 735us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 828/1000\n",
      "10/10 [==============================] - 0s 764us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 829/1000\n",
      "10/10 [==============================] - 0s 705us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 830/1000\n",
      "10/10 [==============================] - 0s 734us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 831/1000\n",
      "10/10 [==============================] - 0s 703us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 832/1000\n",
      "10/10 [==============================] - 0s 738us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 833/1000\n",
      "10/10 [==============================] - 0s 744us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 834/1000\n",
      "10/10 [==============================] - 0s 661us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 835/1000\n",
      "10/10 [==============================] - 0s 682us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 836/1000\n",
      "10/10 [==============================] - 0s 740us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 837/1000\n",
      "10/10 [==============================] - 0s 730us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 838/1000\n",
      "10/10 [==============================] - 0s 729us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 839/1000\n",
      "10/10 [==============================] - 0s 729us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 840/1000\n",
      "10/10 [==============================] - 0s 740us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 841/1000\n",
      "10/10 [==============================] - 0s 698us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 842/1000\n",
      "10/10 [==============================] - 0s 715us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 843/1000\n",
      "10/10 [==============================] - 0s 774us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 844/1000\n",
      "10/10 [==============================] - 0s 711us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 845/1000\n",
      "10/10 [==============================] - 0s 747us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 846/1000\n",
      "10/10 [==============================] - 0s 721us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 847/1000\n",
      "10/10 [==============================] - 0s 685us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 848/1000\n",
      "10/10 [==============================] - 0s 694us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 849/1000\n",
      "10/10 [==============================] - 0s 769us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 850/1000\n",
      "10/10 [==============================] - 0s 733us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 851/1000\n",
      "10/10 [==============================] - 0s 698us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 852/1000\n",
      "10/10 [==============================] - 0s 705us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 853/1000\n",
      "10/10 [==============================] - 0s 712us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 854/1000\n",
      "10/10 [==============================] - 0s 766us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 855/1000\n",
      "10/10 [==============================] - 0s 765us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 856/1000\n",
      "10/10 [==============================] - 0s 792us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 857/1000\n",
      "10/10 [==============================] - 0s 735us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 858/1000\n",
      "10/10 [==============================] - 0s 760us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 859/1000\n",
      "10/10 [==============================] - 0s 741us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 860/1000\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 861/1000\n",
      "10/10 [==============================] - 0s 929us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 862/1000\n",
      "10/10 [==============================] - 0s 753us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 863/1000\n",
      "10/10 [==============================] - 0s 692us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 864/1000\n",
      "10/10 [==============================] - 0s 701us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 865/1000\n",
      "10/10 [==============================] - 0s 757us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 866/1000\n",
      "10/10 [==============================] - 0s 696us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 867/1000\n",
      "10/10 [==============================] - 0s 753us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 868/1000\n",
      "10/10 [==============================] - 0s 733us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 869/1000\n",
      "10/10 [==============================] - 0s 735us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 870/1000\n",
      "10/10 [==============================] - 0s 740us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 871/1000\n",
      "10/10 [==============================] - 0s 711us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 872/1000\n",
      "10/10 [==============================] - 0s 740us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 873/1000\n",
      "10/10 [==============================] - 0s 767us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 874/1000\n",
      "10/10 [==============================] - 0s 770us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 875/1000\n",
      "10/10 [==============================] - 0s 676us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 876/1000\n",
      "10/10 [==============================] - 0s 800us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 877/1000\n",
      "10/10 [==============================] - 0s 766us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 878/1000\n",
      "10/10 [==============================] - 0s 733us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 879/1000\n",
      "10/10 [==============================] - 0s 726us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 880/1000\n",
      "10/10 [==============================] - 0s 774us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 881/1000\n",
      "10/10 [==============================] - 0s 753us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 882/1000\n",
      "10/10 [==============================] - 0s 724us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 883/1000\n",
      "10/10 [==============================] - 0s 759us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 884/1000\n",
      "10/10 [==============================] - 0s 737us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 885/1000\n",
      "10/10 [==============================] - 0s 782us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 886/1000\n",
      "10/10 [==============================] - 0s 783us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 887/1000\n",
      "10/10 [==============================] - 0s 729us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 888/1000\n",
      "10/10 [==============================] - 0s 759us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 889/1000\n",
      "10/10 [==============================] - 0s 772us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 890/1000\n",
      "10/10 [==============================] - 0s 707us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 891/1000\n",
      "10/10 [==============================] - 0s 737us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 892/1000\n",
      "10/10 [==============================] - 0s 748us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 893/1000\n",
      "10/10 [==============================] - 0s 760us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 894/1000\n",
      "10/10 [==============================] - 0s 774us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 895/1000\n",
      "10/10 [==============================] - 0s 726us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 896/1000\n",
      "10/10 [==============================] - 0s 777us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 897/1000\n",
      "10/10 [==============================] - 0s 806us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 898/1000\n",
      "10/10 [==============================] - 0s 784us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 899/1000\n",
      "10/10 [==============================] - 0s 732us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 900/1000\n",
      "10/10 [==============================] - 0s 731us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 901/1000\n",
      "10/10 [==============================] - 0s 740us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 902/1000\n",
      "10/10 [==============================] - 0s 694us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 903/1000\n",
      "10/10 [==============================] - 0s 715us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 904/1000\n",
      "10/10 [==============================] - 0s 751us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 905/1000\n",
      "10/10 [==============================] - 0s 770us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 906/1000\n",
      "10/10 [==============================] - 0s 782us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 907/1000\n",
      "10/10 [==============================] - 0s 777us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 908/1000\n",
      "10/10 [==============================] - 0s 766us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 909/1000\n",
      "10/10 [==============================] - 0s 745us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 910/1000\n",
      "10/10 [==============================] - 0s 694us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 911/1000\n",
      "10/10 [==============================] - 0s 766us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 912/1000\n",
      "10/10 [==============================] - 0s 711us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 913/1000\n",
      "10/10 [==============================] - 0s 763us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 914/1000\n",
      "10/10 [==============================] - 0s 791us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 915/1000\n",
      "10/10 [==============================] - 0s 736us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 916/1000\n",
      "10/10 [==============================] - 0s 698us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 917/1000\n",
      "10/10 [==============================] - 0s 746us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 918/1000\n",
      "10/10 [==============================] - 0s 705us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 919/1000\n",
      "10/10 [==============================] - 0s 738us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 920/1000\n",
      "10/10 [==============================] - 0s 742us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 921/1000\n",
      "10/10 [==============================] - 0s 739us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 922/1000\n",
      "10/10 [==============================] - 0s 744us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 923/1000\n",
      "10/10 [==============================] - 0s 757us/step - loss: 0.2022 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 924/1000\n",
      "10/10 [==============================] - 0s 741us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 925/1000\n",
      "10/10 [==============================] - 0s 714us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 926/1000\n",
      "10/10 [==============================] - 0s 737us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 927/1000\n",
      "10/10 [==============================] - 0s 806us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 928/1000\n",
      "10/10 [==============================] - 0s 787us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 929/1000\n",
      "10/10 [==============================] - 0s 682us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 930/1000\n",
      "10/10 [==============================] - 0s 785us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 931/1000\n",
      "10/10 [==============================] - 0s 730us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 932/1000\n",
      "10/10 [==============================] - 0s 755us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 933/1000\n",
      "10/10 [==============================] - 0s 731us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 934/1000\n",
      "10/10 [==============================] - 0s 781us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 935/1000\n",
      "10/10 [==============================] - 0s 703us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 936/1000\n",
      "10/10 [==============================] - 0s 684us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 937/1000\n",
      "10/10 [==============================] - 0s 689us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 938/1000\n",
      "10/10 [==============================] - 0s 713us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 939/1000\n",
      "10/10 [==============================] - 0s 729us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 940/1000\n",
      "10/10 [==============================] - 0s 721us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 941/1000\n",
      "10/10 [==============================] - 0s 782us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 942/1000\n",
      "10/10 [==============================] - 0s 727us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 943/1000\n",
      "10/10 [==============================] - 0s 776us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 944/1000\n",
      "10/10 [==============================] - 0s 725us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 945/1000\n",
      "10/10 [==============================] - 0s 722us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 946/1000\n",
      "10/10 [==============================] - 0s 715us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 947/1000\n",
      "10/10 [==============================] - 0s 790us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 948/1000\n",
      "10/10 [==============================] - 0s 750us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 949/1000\n",
      "10/10 [==============================] - 0s 712us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 950/1000\n",
      "10/10 [==============================] - 0s 757us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 951/1000\n",
      "10/10 [==============================] - 0s 750us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 952/1000\n",
      "10/10 [==============================] - 0s 758us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 953/1000\n",
      "10/10 [==============================] - 0s 723us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 954/1000\n",
      "10/10 [==============================] - 0s 732us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 955/1000\n",
      "10/10 [==============================] - 0s 717us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 956/1000\n",
      "10/10 [==============================] - 0s 755us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 957/1000\n",
      "10/10 [==============================] - 0s 727us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 958/1000\n",
      "10/10 [==============================] - 0s 751us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 959/1000\n",
      "10/10 [==============================] - 0s 701us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 960/1000\n",
      "10/10 [==============================] - 0s 742us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 961/1000\n",
      "10/10 [==============================] - 0s 759us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 962/1000\n",
      "10/10 [==============================] - 0s 696us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 963/1000\n",
      "10/10 [==============================] - 0s 725us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 964/1000\n",
      "10/10 [==============================] - 0s 718us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 965/1000\n",
      "10/10 [==============================] - 0s 742us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 966/1000\n",
      "10/10 [==============================] - 0s 769us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 967/1000\n",
      "10/10 [==============================] - 0s 725us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 968/1000\n",
      "10/10 [==============================] - 0s 740us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 969/1000\n",
      "10/10 [==============================] - 0s 788us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 970/1000\n",
      "10/10 [==============================] - 0s 727us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 971/1000\n",
      "10/10 [==============================] - 0s 733us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 972/1000\n",
      "10/10 [==============================] - 0s 709us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 973/1000\n",
      "10/10 [==============================] - 0s 771us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 974/1000\n",
      "10/10 [==============================] - 0s 748us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 975/1000\n",
      "10/10 [==============================] - 0s 767us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 976/1000\n",
      "10/10 [==============================] - 0s 743us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 977/1000\n",
      "10/10 [==============================] - 0s 775us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 978/1000\n",
      "10/10 [==============================] - 0s 722us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 979/1000\n",
      "10/10 [==============================] - 0s 724us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 980/1000\n",
      "10/10 [==============================] - 0s 745us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 981/1000\n",
      "10/10 [==============================] - 0s 687us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 982/1000\n",
      "10/10 [==============================] - 0s 710us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 983/1000\n",
      "10/10 [==============================] - 0s 708us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 984/1000\n",
      "10/10 [==============================] - 0s 725us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 985/1000\n",
      "10/10 [==============================] - 0s 762us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 986/1000\n",
      "10/10 [==============================] - 0s 776us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 987/1000\n",
      "10/10 [==============================] - 0s 742us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 988/1000\n",
      "10/10 [==============================] - 0s 721us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 989/1000\n",
      "10/10 [==============================] - 0s 714us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 990/1000\n",
      "10/10 [==============================] - 0s 770us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 991/1000\n",
      "10/10 [==============================] - 0s 742us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 992/1000\n",
      "10/10 [==============================] - 0s 740us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 993/1000\n",
      "10/10 [==============================] - 0s 780us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 994/1000\n",
      "10/10 [==============================] - 0s 686us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 995/1000\n",
      "10/10 [==============================] - 0s 781us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 996/1000\n",
      "10/10 [==============================] - 0s 745us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 997/1000\n",
      "10/10 [==============================] - 0s 740us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 998/1000\n",
      "10/10 [==============================] - 0s 768us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 999/1000\n",
      "10/10 [==============================] - 0s 739us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "Epoch 1000/1000\n",
      "10/10 [==============================] - 0s 744us/step - loss: 0.2020 - accuracy: 0.7980 - precision: 0.7403 - recall: 0.6351\n",
      "{'loss': [0.5031108260154724, 0.49292171001434326, 0.48466551303863525, 0.476540744304657, 0.46865203976631165, 0.46084535121917725, 0.4528578519821167, 0.4448694884777069, 0.43719610571861267, 0.42957013845443726, 0.4221891164779663, 0.4151395559310913, 0.40830543637275696, 0.40174752473831177, 0.3955463469028473, 0.3897201120853424, 0.38411739468574524, 0.3786872327327728, 0.3735016882419586, 0.3685729205608368, 0.3639388084411621, 0.3593994677066803, 0.3551299273967743, 0.3509790599346161, 0.3470679819583893, 0.343185156583786, 0.339403361082077, 0.3358622193336487, 0.33246082067489624, 0.3291386365890503, 0.325847864151001, 0.3227127492427826, 0.3196890354156494, 0.31676819920539856, 0.31393319368362427, 0.31107160449028015, 0.30839285254478455, 0.3056788146495819, 0.3030693531036377, 0.3006320893764496, 0.29812759160995483, 0.29581528902053833, 0.29354041814804077, 0.29132527112960815, 0.2892100214958191, 0.2871636152267456, 0.28526145219802856, 0.2834702432155609, 0.2817182242870331, 0.28001242876052856, 0.27829644083976746, 0.27672335505485535, 0.2752075493335724, 0.2739010453224182, 0.27228328585624695, 0.2709020972251892, 0.26974722743034363, 0.26826897263526917, 0.267007052898407, 0.2657589316368103, 0.2647204101085663, 0.2634052336215973, 0.2622811198234558, 0.2609918713569641, 0.2599632740020752, 0.2588918209075928, 0.25771814584732056, 0.2567110061645508, 0.2556447982788086, 0.2545974850654602, 0.25365689396858215, 0.2526440918445587, 0.25173988938331604, 0.2506818473339081, 0.24962832033634186, 0.24869324266910553, 0.24788948893547058, 0.24683183431625366, 0.2460762858390808, 0.24521373212337494, 0.244436576962471, 0.24365627765655518, 0.24308903515338898, 0.24223268032073975, 0.24154317378997803, 0.24086421728134155, 0.24036389589309692, 0.23976001143455505, 0.239059180021286, 0.23850971460342407, 0.23806487023830414, 0.23742350935935974, 0.23705187439918518, 0.23653076589107513, 0.23608681559562683, 0.2355273813009262, 0.23504944145679474, 0.23473972082138062, 0.23424895107746124, 0.23394496738910675, 0.23330864310264587, 0.23286210000514984, 0.23260381817817688, 0.23226894438266754, 0.23172707855701447, 0.23133255541324615, 0.2310524433851242, 0.23053604364395142, 0.2303546518087387, 0.23014381527900696, 0.22964216768741608, 0.22919295728206635, 0.2288876324892044, 0.22878111898899078, 0.22827312350273132, 0.2279941886663437, 0.22780457139015198, 0.22764988243579865, 0.22714635729789734, 0.22680743038654327, 0.22671237587928772, 0.2264622449874878, 0.226209357380867, 0.22589948773384094, 0.22559145092964172, 0.2253512442111969, 0.22540566325187683, 0.22487933933734894, 0.22457915544509888, 0.22419537603855133, 0.2242712527513504, 0.22405126690864563, 0.22367337346076965, 0.22359567880630493, 0.22339731454849243, 0.22338056564331055, 0.2228785902261734, 0.22277715802192688, 0.22258903086185455, 0.22223979234695435, 0.22225245833396912, 0.22198598086833954, 0.2219061553478241, 0.22173717617988586, 0.22153975069522858, 0.22123514115810394, 0.2211756557226181, 0.22093871235847473, 0.22086560726165771, 0.22057978808879852, 0.22037886083126068, 0.2202281951904297, 0.2201969474554062, 0.2200622260570526, 0.2197798639535904, 0.21958301961421967, 0.21947070956230164, 0.21926411986351013, 0.2196759432554245, 0.2190188467502594, 0.2188793569803238, 0.21881550550460815, 0.2186279296875, 0.218661829829216, 0.21833492815494537, 0.2182801514863968, 0.21821340918540955, 0.21817933022975922, 0.217678502202034, 0.21765810251235962, 0.21756748855113983, 0.21749095618724823, 0.2173786461353302, 0.21730883419513702, 0.2170708179473877, 0.2172985076904297, 0.21684272587299347, 0.21660256385803223, 0.2167070358991623, 0.21674060821533203, 0.2162216156721115, 0.21637260913848877, 0.21619053184986115, 0.21616590023040771, 0.216032475233078, 0.21597599983215332, 0.21582110226154327, 0.21564732491970062, 0.21536482870578766, 0.21561789512634277, 0.2153431624174118, 0.21542415022850037, 0.21515946090221405, 0.21501877903938293, 0.2149016559123993, 0.21494293212890625, 0.21472637355327606, 0.21448911726474762, 0.21464058756828308, 0.21435236930847168, 0.21418724954128265, 0.21403634548187256, 0.21401278674602509, 0.21408085525035858, 0.21380513906478882, 0.21368806064128876, 0.21352024376392365, 0.21349561214447021, 0.21318727731704712, 0.21320365369319916, 0.21334828436374664, 0.21297961473464966, 0.21306860446929932, 0.2125280648469925, 0.21299979090690613, 0.21248991787433624, 0.2124420553445816, 0.2126845121383667, 0.21244554221630096, 0.2122657299041748, 0.21240679919719696, 0.21219347417354584, 0.21202512085437775, 0.21204105019569397, 0.21203669905662537, 0.21197132766246796, 0.21175585687160492, 0.21158726513385773, 0.21154069900512695, 0.2115745097398758, 0.21148532629013062, 0.21138790249824524, 0.2111654132604599, 0.21118152141571045, 0.2111627757549286, 0.21113291382789612, 0.21104824542999268, 0.21068896353244781, 0.2109278291463852, 0.21081170439720154, 0.21062253415584564, 0.2106797695159912, 0.21065108478069305, 0.21045249700546265, 0.2105003148317337, 0.2101188451051712, 0.21021516621112823, 0.21037623286247253, 0.2103644460439682, 0.21011854708194733, 0.2102549523115158, 0.20990976691246033, 0.21011805534362793, 0.20973500609397888, 0.20976391434669495, 0.20982342958450317, 0.20950961112976074, 0.20946726202964783, 0.2096138596534729, 0.20938773453235626, 0.20920194685459137, 0.20925761759281158, 0.20917664468288422, 0.20883916318416595, 0.20912764966487885, 0.20898258686065674, 0.2088956981897354, 0.20886953175067902, 0.2085888385772705, 0.20885448157787323, 0.20887072384357452, 0.20866316556930542, 0.2085956484079361, 0.2083953320980072, 0.20838160812854767, 0.20856866240501404, 0.20818403363227844, 0.20801673829555511, 0.20842458307743073, 0.20818690955638885, 0.20833082497119904, 0.20784541964530945, 0.20772811770439148, 0.20790700614452362, 0.20832256972789764, 0.20772585272789001, 0.20735600590705872, 0.20771436393260956, 0.20753347873687744, 0.20752312242984772, 0.20759010314941406, 0.207387313246727, 0.20750248432159424, 0.20734903216362, 0.20765405893325806, 0.20690646767616272, 0.20707117021083832, 0.20738649368286133, 0.2073964923620224, 0.20724160969257355, 0.20701129734516144, 0.20702947676181793, 0.20714171230793, 0.20685699582099915, 0.20698139071464539, 0.20660990476608276, 0.2068936824798584, 0.20668460428714752, 0.2066439688205719, 0.2066376805305481, 0.20636774599552155, 0.20654813945293427, 0.20633016526699066, 0.20687656104564667, 0.20638196170330048, 0.20630063116550446, 0.2061508744955063, 0.20632123947143555, 0.2061958760023117, 0.20629948377609253, 0.20588919520378113, 0.20599383115768433, 0.20637498795986176, 0.20611535012722015, 0.2059178650379181, 0.20603154599666595, 0.20597422122955322, 0.20579637587070465, 0.20581459999084473, 0.2057875096797943, 0.2061099410057068, 0.20570158958435059, 0.205649733543396, 0.20561155676841736, 0.205454021692276, 0.20566606521606445, 0.20587192475795746, 0.2056758999824524, 0.20545434951782227, 0.20532147586345673, 0.20511111617088318, 0.2055015116930008, 0.20578116178512573, 0.20544812083244324, 0.20492900907993317, 0.20531408488750458, 0.20495307445526123, 0.2052515596151352, 0.20538091659545898, 0.20506176352500916, 0.20488958060741425, 0.2050110250711441, 0.20530396699905396, 0.20492632687091827, 0.2048368602991104, 0.20472592115402222, 0.2047736644744873, 0.20472444593906403, 0.20477178692817688, 0.20469604432582855, 0.20444266498088837, 0.20480918884277344, 0.20461277663707733, 0.2044319361448288, 0.20483911037445068, 0.204608753323555, 0.20442412793636322, 0.20465506613254547, 0.20440250635147095, 0.2045871466398239, 0.20462630689144135, 0.20458263158798218, 0.2043110728263855, 0.2042895406484604, 0.20422643423080444, 0.20449315011501312, 0.20436210930347443, 0.20391669869422913, 0.20430119335651398, 0.20443548262119293, 0.20419590175151825, 0.20405897498130798, 0.20413389801979065, 0.20414356887340546, 0.20399869978427887, 0.20398379862308502, 0.2039240151643753, 0.20413218438625336, 0.20424684882164001, 0.20396529138088226, 0.2038770318031311, 0.20408429205417633, 0.20386172831058502, 0.20384809374809265, 0.2038290947675705, 0.20370683073997498, 0.20366817712783813, 0.20368055999279022, 0.2036409229040146, 0.20403644442558289, 0.20340098440647125, 0.20345744490623474, 0.20427581667900085, 0.20369072258472443, 0.20342625677585602, 0.20333760976791382, 0.2033860832452774, 0.20376883447170258, 0.2034093141555786, 0.20341405272483826, 0.20380990207195282, 0.2033892273902893, 0.20333342254161835, 0.20367249846458435, 0.20332838594913483, 0.20387844741344452, 0.20325054228305817, 0.20313893258571625, 0.20358288288116455, 0.20325900614261627, 0.20322203636169434, 0.2035042643547058, 0.20327717065811157, 0.20325103402137756, 0.20337875187397003, 0.20323696732521057, 0.20315824449062347, 0.20301783084869385, 0.2035832554101944, 0.2029346078634262, 0.2032831758260727, 0.2034701108932495, 0.2032604068517685, 0.20299002528190613, 0.20297616720199585, 0.20308829843997955, 0.20341408252716064, 0.20302166044712067, 0.20292115211486816, 0.2030409425497055, 0.2030312865972519, 0.20303310453891754, 0.20323719084262848, 0.20283187925815582, 0.2031150907278061, 0.20287293195724487, 0.20274290442466736, 0.20324808359146118, 0.20300842821598053, 0.20290692150592804, 0.20296095311641693, 0.2030545324087143, 0.20281076431274414, 0.2028740793466568, 0.2030949890613556, 0.20279748737812042, 0.20296990871429443, 0.20262089371681213, 0.2028198540210724, 0.20272035896778107, 0.20279225707054138, 0.20265071094036102, 0.20320197939872742, 0.2027595043182373, 0.20263983309268951, 0.20277924835681915, 0.2027316838502884, 0.20264379680156708, 0.20311377942562103, 0.20258395373821259, 0.2026178538799286, 0.20255716145038605, 0.20268014073371887, 0.20278587937355042, 0.20255301892757416, 0.202851340174675, 0.20267318189144135, 0.20246921479701996, 0.20264986157417297, 0.2026572972536087, 0.20262832939624786, 0.2024485170841217, 0.20260469615459442, 0.20267842710018158, 0.20256300270557404, 0.20250825583934784, 0.2025451362133026, 0.20247407257556915, 0.2027178555727005, 0.20241603255271912, 0.20303842425346375, 0.2024458348751068, 0.2023315131664276, 0.20239309966564178, 0.20251433551311493, 0.20255105197429657, 0.20238670706748962, 0.2025068998336792, 0.20254801213741302, 0.20234200358390808, 0.2023887038230896, 0.20237264037132263, 0.20255833864212036, 0.20236867666244507, 0.20245622098445892, 0.20229370892047882, 0.20244888961315155, 0.20263773202896118, 0.2023448795080185, 0.20227135717868805, 0.20234595239162445, 0.20276831090450287, 0.20237401127815247, 0.20226679742336273, 0.20228871703147888, 0.20228227972984314, 0.20239444077014923, 0.202394038438797, 0.20228047668933868, 0.20234818756580353, 0.2023252248764038, 0.20229275524616241, 0.20225760340690613, 0.20230859518051147, 0.20240136981010437, 0.2021721452474594, 0.2023586928844452, 0.20231130719184875, 0.2023150622844696, 0.2021816223859787, 0.20233915746212006, 0.2022024691104889, 0.20232629776000977, 0.20218591392040253, 0.20221687853336334, 0.2023923248052597, 0.20216315984725952, 0.2022276073694229, 0.2021830826997757, 0.2022123485803604, 0.20236743986606598, 0.20218977332115173, 0.20219014585018158, 0.2021804004907608, 0.2023087590932846, 0.20222648978233337, 0.2023092806339264, 0.20210681855678558, 0.2020994871854782, 0.20213225483894348, 0.20223557949066162, 0.2022094875574112, 0.2021082490682602, 0.20214490592479706, 0.20218361914157867, 0.20217330753803253, 0.20224742591381073, 0.20214952528476715, 0.2020828276872635, 0.2021394819021225, 0.20243355631828308, 0.20208220183849335, 0.20207639038562775, 0.20216359198093414, 0.20226740837097168, 0.20206309854984283, 0.20208722352981567, 0.2020890861749649, 0.20214281976222992, 0.20211219787597656, 0.2021583616733551, 0.20217913389205933, 0.2020755410194397, 0.20206952095031738, 0.20218703150749207, 0.20209667086601257, 0.20211540162563324, 0.2021707445383072, 0.2020803987979889, 0.2020985335111618, 0.20209865272045135, 0.20221760869026184, 0.20217233896255493, 0.2020489126443863, 0.20204278826713562, 0.20205019414424896, 0.20205949246883392, 0.20231030881404877, 0.2020311951637268, 0.20203691720962524, 0.20203503966331482, 0.2020692527294159, 0.20220930874347687, 0.20204156637191772, 0.20202550292015076, 0.20218519866466522, 0.2020537108182907, 0.20203498005867004, 0.20207399129867554, 0.20203562080860138, 0.20235446095466614, 0.20203404128551483, 0.20202408730983734, 0.2020239382982254, 0.2020188421010971, 0.20204736292362213, 0.20206096768379211, 0.20216496288776398, 0.20201241970062256, 0.2020193189382553, 0.20201925933361053, 0.2020064741373062, 0.20229816436767578, 0.20209605991840363, 0.20201197266578674, 0.2020048201084137, 0.20200687646865845, 0.20201383531093597, 0.2021205872297287, 0.20209816098213196, 0.20200884342193604, 0.2020024210214615, 0.20200115442276, 0.2020220160484314, 0.20201461017131805, 0.2020808607339859, 0.20203983783721924, 0.20199452340602875, 0.20205837488174438, 0.20203186571598053, 0.20200620591640472, 0.2020033597946167, 0.2020101696252823, 0.2020798921585083, 0.20200200378894806, 0.20200690627098083, 0.20200355350971222, 0.20199282467365265, 0.20274323225021362, 0.2020396739244461, 0.20201371610164642, 0.2020011991262436, 0.2019977569580078, 0.2019919455051422, 0.2019859105348587, 0.201990008354187, 0.20198902487754822, 0.2020758092403412, 0.20200799405574799, 0.20201390981674194, 0.2020075023174286, 0.20198720693588257, 0.20198507606983185, 0.20202311873435974, 0.20201051235198975, 0.20205293595790863, 0.2020469605922699, 0.20198379456996918, 0.20197905600070953, 0.20198017358779907, 0.20198556780815125, 0.20202161371707916, 0.20201542973518372, 0.202025905251503, 0.2019755244255066, 0.20197567343711853, 0.20199930667877197, 0.20210887491703033, 0.20199641585350037, 0.20198063552379608, 0.20197558403015137, 0.20197409391403198, 0.20197944343090057, 0.20198608934879303, 0.2029125988483429, 0.20200464129447937, 0.20199452340602875, 0.20198538899421692, 0.20197847485542297, 0.2019749879837036, 0.20197133719921112, 0.2019721418619156, 0.20197410881519318, 0.20197974145412445, 0.2020016312599182, 0.20197463035583496, 0.20198668539524078, 0.20199568569660187, 0.20203706622123718, 0.20197442173957825, 0.2019684910774231, 0.20196868479251862, 0.20197588205337524, 0.20197707414627075, 0.20198331773281097, 0.2020762711763382, 0.20196998119354248, 0.20196671783924103, 0.20196840167045593, 0.2019677609205246, 0.2019776999950409, 0.20202220976352692, 0.20204412937164307, 0.2019701451063156, 0.20196793973445892, 0.20196644961833954, 0.20196521282196045, 0.2019655406475067, 0.2019677311182022, 0.20198234915733337, 0.20196956396102905, 0.2020624577999115, 0.20196565985679626, 0.20196393132209778, 0.20196302235126495, 0.20196522772312164, 0.20197641849517822, 0.20196589827537537, 0.20204214751720428, 0.20196300745010376, 0.20196251571178436, 0.20196253061294556, 0.20196323096752167, 0.20197439193725586, 0.20197784900665283, 0.20197147130966187, 0.2019660323858261, 0.20197314023971558, 0.20199519395828247, 0.20196552574634552, 0.20196117460727692, 0.20196124911308289, 0.20196543633937836, 0.20198646187782288, 0.20197920501232147, 0.2019624412059784, 0.2019607275724411, 0.20196090638637543, 0.20197373628616333, 0.20197297632694244, 0.20196132361888885, 0.20200511813163757, 0.2019621580839157, 0.2019607573747635, 0.20196028053760529, 0.20196275413036346, 0.20198342204093933, 0.2019776552915573, 0.20197694003582, 0.20196020603179932, 0.20195968449115753, 0.20196038484573364, 0.20196759700775146, 0.201975017786026, 0.2019602209329605, 0.20195908844470978, 0.20197148621082306, 0.20197537541389465, 0.20196853578090668, 0.20196063816547394, 0.20195916295051575, 0.20195941627025604, 0.20196060836315155, 0.20196783542633057, 0.2019777148962021, 0.20195947587490082, 0.20195867121219635, 0.20199890434741974, 0.2019764930009842, 0.2019597440958023, 0.20195916295051575, 0.20195868611335754, 0.20195823907852173, 0.20195768773555756, 0.2019585520029068, 0.20199039578437805, 0.20196513831615448, 0.20195868611335754, 0.20195798575878143, 0.20195792615413666, 0.20195801556110382, 0.20195895433425903, 0.20197038352489471, 0.2020801305770874, 0.20196083188056946, 0.20195931196212769, 0.20195841789245605, 0.2019577920436859, 0.20195750892162323, 0.20195725560188293, 0.20195695757865906, 0.20195861160755157, 0.20197933912277222, 0.2019592970609665, 0.20195741951465607, 0.20195677876472473, 0.2019650638103485, 0.20198886096477509, 0.20196114480495453, 0.2019588053226471, 0.20195786654949188, 0.20195725560188293, 0.20195716619491577, 0.20195750892162323, 0.20196101069450378, 0.20215646922588348, 0.20196400582790375, 0.20196163654327393, 0.2019599974155426, 0.20195864140987396, 0.2019578069448471, 0.20195722579956055, 0.20195691287517548, 0.20195668935775757, 0.2019568234682083, 0.20195652544498444, 0.20196102559566498, 0.2019774168729782, 0.2019563913345337, 0.20195616781711578, 0.201956108212471, 0.20195628702640533, 0.20195654034614563, 0.20197232067584991, 0.20197109878063202, 0.20195646584033966, 0.20195604860782623, 0.20195600390434265, 0.20195604860782623, 0.201956108212471, 0.20195642113685608, 0.201970174908638, 0.20196197926998138, 0.20195624232292175, 0.20195594429969788, 0.2019558846950531, 0.2019616812467575, 0.20196090638637543, 0.20195649564266205, 0.20195601880550385, 0.2019558697938919, 0.20195721089839935, 0.20195823907852173, 0.20196983218193054, 0.20199020206928253, 0.20195762813091278, 0.20195674896240234, 0.20195625722408295, 0.20195601880550385, 0.20195575058460236, 0.2019556313753128, 0.20195572078227997, 0.20195569097995758, 0.201961949467659, 0.2019621729850769, 0.201961487531662, 0.20195546746253967, 0.20195549726486206, 0.20195548236370087, 0.20195595920085907, 0.2019655555486679, 0.20196007192134857, 0.20195642113685608, 0.20195603370666504, 0.201955646276474, 0.20195546746253967, 0.2019554227590561, 0.20196613669395447, 0.2019568830728531, 0.20195560157299042, 0.20195534825325012, 0.2019554078578949, 0.20195528864860535, 0.20195983350276947, 0.2019629031419754, 0.2019553780555725, 0.20195510983467102, 0.20195509493350983, 0.2019551545381546, 0.20195689797401428, 0.2019670009613037, 0.2019556164741516, 0.20195534825325012, 0.20195531845092773, 0.20195522904396057, 0.2019577920436859, 0.20197613537311554, 0.2019554227590561, 0.20195521414279938, 0.20195512473583221, 0.20195505023002625, 0.20195505023002625, 0.20195499062538147, 0.2019563913345337, 0.20195887982845306, 0.20195671916007996, 0.20195508003234863, 0.20195543766021729, 0.20195519924163818, 0.20195719599723816, 0.20195549726486206, 0.20195505023002625, 0.20195512473583221, 0.20199500024318695, 0.20196084678173065, 0.20195691287517548, 0.20195606350898743, 0.2019556313753128, 0.20195530354976654, 0.201955184340477, 0.20195499062538147, 0.2019549459218979, 0.2019549459218979, 0.20195505023002625, 0.20196427404880524, 0.2019566148519516, 0.20195508003234863, 0.2019549012184143, 0.20195482671260834, 0.20195481181144714, 0.2019549012184143, 0.20195512473583221, 0.20224228501319885, 0.2019578218460083, 0.2019575834274292, 0.20195722579956055, 0.20195676386356354, 0.2019563615322113, 0.20195592939853668, 0.20195560157299042, 0.20195536315441132, 0.201955184340477, 0.20195502042770386, 0.20195497572422028, 0.20195484161376953, 0.20195478200912476, 0.20195479691028595, 0.20195621252059937, 0.20196318626403809, 0.20195546746253967, 0.20195524394512177, 0.20195497572422028, 0.2019549161195755, 0.20195482671260834, 0.20195472240447998, 0.20195485651493073, 0.2019597887992859, 0.20195578038692474, 0.20195499062538147, 0.20195473730564117, 0.20195472240447998, 0.20195473730564117, 0.20195475220680237, 0.20195484161376953, 0.2019646018743515, 0.20195595920085907, 0.20195484161376953, 0.20195476710796356, 0.20195472240447998, 0.2019546926021576, 0.2019546627998352, 0.20195463299751282, 0.2019546926021576, 0.20195479691028595, 0.20196036994457245, 0.20196110010147095, 0.20195484161376953, 0.2019546777009964, 0.20195461809635162, 0.20195460319519043, 0.20195458829402924, 0.20195460319519043, 0.20195461809635162, 0.20195560157299042, 0.20196041464805603, 0.20195506513118744, 0.20195476710796356, 0.20195472240447998, 0.20195463299751282, 0.20195460319519043, 0.201954647898674, 0.20195816457271576, 0.20195598900318146, 0.20195476710796356, 0.2019546777009964, 0.20195460319519043, 0.20195460319519043, 0.20195463299751282, 0.20195560157299042, 0.20195528864860535, 0.20195697247982025, 0.20195525884628296, 0.2019546627998352, 0.20195460319519043, 0.20195460319519043, 0.20195496082305908, 0.20195557177066803, 0.2019597440958023, 0.2019546777009964, 0.201954647898674], 'accuracy': [0.47557002305984497, 0.5114006400108337, 0.5358306169509888, 0.5618892312049866, 0.5814332365989685, 0.6009771823883057, 0.6237785220146179, 0.6302931308746338, 0.653094470500946, 0.6742671132087708, 0.6872963905334473, 0.6905537247657776, 0.7003257274627686, 0.7035830616950989, 0.7149837017059326, 0.7182410359382629, 0.7247557044029236, 0.7198697328567505, 0.7263843417167664, 0.723127007484436, 0.7247557044029236, 0.7247557044029236, 0.7312703728675842, 0.7361563444137573, 0.7377850413322449, 0.7361563444137573, 0.7345277070999146, 0.732899010181427, 0.7361563444137573, 0.7361563444137573, 0.7377850413322449, 0.7361563444137573, 0.7345277070999146, 0.7394136786460876, 0.7361563444137573, 0.7377850413322449, 0.7394136786460876, 0.7361563444137573, 0.7377850413322449, 0.742671012878418, 0.7442996501922607, 0.7442996501922607, 0.7475569844245911, 0.7491856813430786, 0.7557003498077393, 0.757328987121582, 0.7589576840400696, 0.7622149586677551, 0.767100989818573, 0.7638436555862427, 0.7638436555862427, 0.7622149586677551, 0.7605863213539124, 0.7622149586677551, 0.7622149586677551, 0.7622149586677551, 0.7622149586677551, 0.7654722929000854, 0.7654722929000854, 0.7638436555862427, 0.7654722929000854, 0.767100989818573, 0.767100989818573, 0.7687296271324158, 0.7703583240509033, 0.7687296271324158, 0.7687296271324158, 0.767100989818573, 0.767100989818573, 0.7703583240509033, 0.7687296271324158, 0.7703583240509033, 0.7703583240509033, 0.7736156582832336, 0.7736156582832336, 0.7736156582832336, 0.7736156582832336, 0.776872992515564, 0.7752442955970764, 0.7736156582832336, 0.7736156582832336, 0.7785016298294067, 0.776872992515564, 0.776872992515564, 0.7785016298294067, 0.7785016298294067, 0.7785016298294067, 0.7752442955970764, 0.7801302671432495, 0.7817589640617371, 0.7817589640617371, 0.7785016298294067, 0.7785016298294067, 0.7785016298294067, 0.776872992515564, 0.7785016298294067, 0.776872992515564, 0.7752442955970764, 0.7785016298294067, 0.776872992515564, 0.7801302671432495, 0.7801302671432495, 0.7801302671432495, 0.7801302671432495, 0.7801302671432495, 0.7801302671432495, 0.7817589640617371, 0.7833876013755798, 0.7817589640617371, 0.7833876013755798, 0.7817589640617371, 0.7850162982940674, 0.7850162982940674, 0.7850162982940674, 0.7833876013755798, 0.7850162982940674, 0.7866449356079102, 0.7866449356079102, 0.7866449356079102, 0.7850162982940674, 0.7866449356079102, 0.7850162982940674, 0.7882736325263977, 0.7866449356079102, 0.7882736325263977, 0.7882736325263977, 0.7882736325263977, 0.7882736325263977, 0.7899022698402405, 0.7882736325263977, 0.7882736325263977, 0.7899022698402405, 0.7882736325263977, 0.7866449356079102, 0.7899022698402405, 0.7882736325263977, 0.7882736325263977, 0.7882736325263977, 0.7882736325263977, 0.7882736325263977, 0.7882736325263977, 0.7882736325263977, 0.7899022698402405, 0.7899022698402405, 0.7882736325263977, 0.7899022698402405, 0.7899022698402405, 0.7882736325263977, 0.7899022698402405, 0.7899022698402405, 0.7899022698402405, 0.7899022698402405, 0.7899022698402405, 0.7882736325263977, 0.7882736325263977, 0.7899022698402405, 0.7899022698402405, 0.7882736325263977, 0.7899022698402405, 0.7899022698402405, 0.7899022698402405, 0.7899022698402405, 0.7899022698402405, 0.7899022698402405, 0.7899022698402405, 0.791530966758728, 0.7882736325263977, 0.7882736325263977, 0.7899022698402405, 0.791530966758728, 0.791530966758728, 0.7899022698402405, 0.791530966758728, 0.791530966758728, 0.791530966758728, 0.791530966758728, 0.791530966758728, 0.791530966758728, 0.791530966758728, 0.7899022698402405, 0.791530966758728, 0.791530966758728, 0.791530966758728, 0.791530966758728, 0.7899022698402405, 0.791530966758728, 0.7899022698402405, 0.791530966758728, 0.7899022698402405, 0.791530966758728, 0.7931596040725708, 0.7931596040725708, 0.7931596040725708, 0.7931596040725708, 0.7899022698402405, 0.791530966758728, 0.791530966758728, 0.7931596040725708, 0.791530966758728, 0.7931596040725708, 0.7947883009910583, 0.7947883009910583, 0.7947883009910583, 0.7947883009910583, 0.7947883009910583, 0.7947883009910583, 0.7947883009910583, 0.7947883009910583, 0.7947883009910583, 0.7947883009910583, 0.7947883009910583, 0.7947883009910583, 0.7947883009910583, 0.7947883009910583, 0.7947883009910583, 0.7947883009910583, 0.7947883009910583, 0.7947883009910583, 0.7947883009910583, 0.7947883009910583, 0.7947883009910583, 0.7947883009910583, 0.7947883009910583, 0.7947883009910583, 0.7947883009910583, 0.7947883009910583, 0.7947883009910583, 0.7964169383049011, 0.7947883009910583, 0.7947883009910583, 0.7947883009910583, 0.7947883009910583, 0.7947883009910583, 0.7947883009910583, 0.7947883009910583, 0.7947883009910583, 0.7947883009910583, 0.7964169383049011, 0.7947883009910583, 0.7964169383049011, 0.7964169383049011, 0.7964169383049011, 0.7964169383049011, 0.7980455756187439, 0.7964169383049011, 0.7980455756187439, 0.7964169383049011, 0.7947883009910583, 0.7947883009910583, 0.7964169383049011, 0.7964169383049011, 0.7964169383049011, 0.7964169383049011, 0.7964169383049011, 0.7964169383049011, 0.7964169383049011, 0.7964169383049011, 0.7964169383049011, 0.7980455756187439, 0.7964169383049011, 0.7980455756187439, 0.7964169383049011, 0.7964169383049011, 0.7964169383049011, 0.7964169383049011, 0.7964169383049011, 0.7980455756187439, 0.7980455756187439, 0.7964169383049011, 0.7964169383049011, 0.7964169383049011, 0.7964169383049011, 0.7964169383049011, 0.7980455756187439, 0.7980455756187439, 0.7964169383049011, 0.7964169383049011, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7964169383049011, 0.7980455756187439, 0.7980455756187439, 0.7964169383049011, 0.7980455756187439, 0.7964169383049011, 0.7980455756187439, 0.7980455756187439, 0.7964169383049011, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7964169383049011, 0.7980455756187439, 0.7964169383049011, 0.7964169383049011, 0.7964169383049011, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7964169383049011, 0.7964169383049011, 0.7980455756187439, 0.7980455756187439, 0.7964169383049011, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7964169383049011, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7964169383049011, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439, 0.7980455756187439], 'precision': [0.30388692021369934, 0.330798476934433, 0.3471074402332306, 0.37168142199516296, 0.39047619700431824, 0.4114583432674408, 0.4462365508079529, 0.45505619049072266, 0.4939759075641632, 0.5354838967323303, 0.563758373260498, 0.5714285969734192, 0.597122311592102, 0.6058394312858582, 0.6343283653259277, 0.641791045665741, 0.6590909361839294, 0.646616518497467, 0.6692913174629211, 0.6614173054695129, 0.6615384817123413, 0.6640625, 0.682539701461792, 0.6959999799728394, 0.7016128897666931, 0.6959999799728394, 0.6875, 0.6793892979621887, 0.6842105388641357, 0.6842105388641357, 0.6865671873092651, 0.6814814805984497, 0.6764705777168274, 0.6808510422706604, 0.6762589812278748, 0.6785714030265808, 0.6783216595649719, 0.6689655184745789, 0.6712328791618347, 0.6778523325920105, 0.6824324131011963, 0.6800000071525574, 0.6818181872367859, 0.6838709712028503, 0.6918238997459412, 0.6913580298423767, 0.6932515501976013, 0.6946107745170593, 0.7023809552192688, 0.6964285969734192, 0.6964285969734192, 0.6900584697723389, 0.6839080452919006, 0.6878612637519836, 0.6857143044471741, 0.6857143044471741, 0.6878612637519836, 0.6936416029930115, 0.691428542137146, 0.6896551847457886, 0.691428542137146, 0.6931818127632141, 0.6954023241996765, 0.6971428394317627, 0.7011494040489197, 0.6971428394317627, 0.6971428394317627, 0.6954023241996765, 0.6954023241996765, 0.7034883499145508, 0.6994219422340393, 0.7034883499145508, 0.7034883499145508, 0.7117646932601929, 0.7093023061752319, 0.7093023061752319, 0.7068965435028076, 0.7151162624359131, 0.7109826803207397, 0.7068965435028076, 0.7068965435028076, 0.7167630195617676, 0.7126436829566956, 0.7126436829566956, 0.7118644118309021, 0.7118644118309021, 0.7142857313156128, 0.7039105892181396, 0.7159090638160706, 0.7200000286102295, 0.7200000286102295, 0.7118644118309021, 0.7118644118309021, 0.7118644118309021, 0.7078651785850525, 0.7118644118309021, 0.7078651785850525, 0.7062146663665771, 0.7094972133636475, 0.7078651785850525, 0.7111111283302307, 0.7111111283302307, 0.7111111283302307, 0.708791196346283, 0.7111111283302307, 0.708791196346283, 0.7103825211524963, 0.7142857313156128, 0.7127071619033813, 0.7142857313156128, 0.7127071619033813, 0.7158470153808594, 0.7158470153808594, 0.7158470153808594, 0.7119565010070801, 0.7158470153808594, 0.717391312122345, 0.717391312122345, 0.717391312122345, 0.7158470153808594, 0.7197802066802979, 0.7158470153808594, 0.7213114500045776, 0.7197802066802979, 0.7213114500045776, 0.7213114500045776, 0.7213114500045776, 0.7213114500045776, 0.7228260636329651, 0.7213114500045776, 0.7213114500045776, 0.7228260636329651, 0.7213114500045776, 0.7197802066802979, 0.7228260636329651, 0.7213114500045776, 0.7213114500045776, 0.7213114500045776, 0.7213114500045776, 0.7213114500045776, 0.7213114500045776, 0.7213114500045776, 0.7228260636329651, 0.7228260636329651, 0.7213114500045776, 0.7228260636329651, 0.7228260636329651, 0.7213114500045776, 0.7228260636329651, 0.7228260636329651, 0.7228260636329651, 0.7228260636329651, 0.7228260636329651, 0.7213114500045776, 0.7213114500045776, 0.7228260636329651, 0.7228260636329651, 0.7213114500045776, 0.7228260636329651, 0.7228260636329651, 0.7228260636329651, 0.7228260636329651, 0.7228260636329651, 0.7228260636329651, 0.7228260636329651, 0.7243243455886841, 0.7213114500045776, 0.7213114500045776, 0.7204301357269287, 0.7243243455886841, 0.7243243455886841, 0.7204301357269287, 0.7243243455886841, 0.7243243455886841, 0.7243243455886841, 0.7243243455886841, 0.7243243455886841, 0.7243243455886841, 0.7243243455886841, 0.7228260636329651, 0.7243243455886841, 0.7243243455886841, 0.7243243455886841, 0.7243243455886841, 0.7204301357269287, 0.7243243455886841, 0.7204301357269287, 0.7243243455886841, 0.7204301357269287, 0.7243243455886841, 0.72826087474823, 0.72826087474823, 0.72826087474823, 0.72826087474823, 0.7204301357269287, 0.7243243455886841, 0.7243243455886841, 0.72826087474823, 0.7243243455886841, 0.72826087474823, 0.7322404384613037, 0.7322404384613037, 0.7322404384613037, 0.7322404384613037, 0.7322404384613037, 0.7322404384613037, 0.7322404384613037, 0.7322404384613037, 0.7322404384613037, 0.7322404384613037, 0.7322404384613037, 0.7322404384613037, 0.7322404384613037, 0.7322404384613037, 0.7322404384613037, 0.7322404384613037, 0.7322404384613037, 0.7322404384613037, 0.7322404384613037, 0.7322404384613037, 0.7322404384613037, 0.7322404384613037, 0.7322404384613037, 0.7322404384613037, 0.7322404384613037, 0.7322404384613037, 0.7322404384613037, 0.7362637519836426, 0.7322404384613037, 0.7322404384613037, 0.7322404384613037, 0.7322404384613037, 0.7322404384613037, 0.7322404384613037, 0.7322404384613037, 0.7322404384613037, 0.7322404384613037, 0.7362637519836426, 0.7322404384613037, 0.7362637519836426, 0.7362637519836426, 0.7362637519836426, 0.7362637519836426, 0.7403314709663391, 0.7362637519836426, 0.7403314709663391, 0.7362637519836426, 0.7348066568374634, 0.7348066568374634, 0.7362637519836426, 0.7362637519836426, 0.7362637519836426, 0.7362637519836426, 0.7362637519836426, 0.7362637519836426, 0.7362637519836426, 0.7362637519836426, 0.7362637519836426, 0.7403314709663391, 0.7362637519836426, 0.7403314709663391, 0.7362637519836426, 0.7362637519836426, 0.7362637519836426, 0.7362637519836426, 0.7362637519836426, 0.7403314709663391, 0.7403314709663391, 0.7362637519836426, 0.7388888597488403, 0.7362637519836426, 0.7388888597488403, 0.7362637519836426, 0.7403314709663391, 0.7403314709663391, 0.7388888597488403, 0.7362637519836426, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7388888597488403, 0.7403314709663391, 0.7403314709663391, 0.7362637519836426, 0.7403314709663391, 0.7362637519836426, 0.7403314709663391, 0.7403314709663391, 0.7362637519836426, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7388888597488403, 0.7403314709663391, 0.7362637519836426, 0.7388888597488403, 0.7362637519836426, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7362637519836426, 0.7388888597488403, 0.7403314709663391, 0.7403314709663391, 0.7388888597488403, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7388888597488403, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7388888597488403, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391, 0.7403314709663391], 'recall': [0.40758293867111206, 0.4123222827911377, 0.3981042802333832, 0.3981042802333832, 0.3886255919933319, 0.3744075894355774, 0.39336493611335754, 0.3838862478733063, 0.3886255919933319, 0.39336493611335754, 0.3981042802333832, 0.3981042802333832, 0.39336493611335754, 0.39336493611335754, 0.4028435945510864, 0.40758293867111206, 0.4123222827911377, 0.40758293867111206, 0.4028435945510864, 0.3981042802333832, 0.40758293867111206, 0.4028435945510864, 0.40758293867111206, 0.4123222827911377, 0.4123222827911377, 0.4123222827911377, 0.41706159710884094, 0.4218009412288666, 0.43127962946891785, 0.43127962946891785, 0.4360189437866211, 0.4360189437866211, 0.4360189437866211, 0.45497629046440125, 0.44549763202667236, 0.450236976146698, 0.4597156345844269, 0.4597156345844269, 0.4644549787044525, 0.47867298126220703, 0.47867298126220703, 0.48341232538223267, 0.4976303279399872, 0.5023696422576904, 0.521327018737793, 0.5308057069778442, 0.5355450510978699, 0.549763023853302, 0.5592417120933533, 0.5545023679733276, 0.5545023679733276, 0.5592417120933533, 0.5639810562133789, 0.5639810562133789, 0.5687204003334045, 0.5687204003334045, 0.5639810562133789, 0.5687204003334045, 0.5734597444534302, 0.5687204003334045, 0.5734597444534302, 0.578199028968811, 0.5734597444534302, 0.578199028968811, 0.578199028968811, 0.578199028968811, 0.578199028968811, 0.5734597444534302, 0.5734597444534302, 0.5734597444534302, 0.5734597444534302, 0.5734597444534302, 0.5734597444534302, 0.5734597444534302, 0.578199028968811, 0.578199028968811, 0.5829383730888367, 0.5829383730888367, 0.5829383730888367, 0.5829383730888367, 0.5829383730888367, 0.5876777172088623, 0.5876777172088623, 0.5876777172088623, 0.5971564054489136, 0.5971564054489136, 0.5924170613288879, 0.5971564054489136, 0.5971564054489136, 0.5971564054489136, 0.5971564054489136, 0.5971564054489136, 0.5971564054489136, 0.5971564054489136, 0.5971564054489136, 0.5971564054489136, 0.5971564054489136, 0.5924170613288879, 0.6018957495689392, 0.5971564054489136, 0.6066350936889648, 0.6066350936889648, 0.6066350936889648, 0.6113743782043457, 0.6066350936889648, 0.6113743782043457, 0.6161137223243713, 0.6161137223243713, 0.6113743782043457, 0.6161137223243713, 0.6113743782043457, 0.620853066444397, 0.620853066444397, 0.620853066444397, 0.620853066444397, 0.620853066444397, 0.6255924105644226, 0.6255924105644226, 0.6255924105644226, 0.620853066444397, 0.620853066444397, 0.620853066444397, 0.6255924105644226, 0.620853066444397, 0.6255924105644226, 0.6255924105644226, 0.6255924105644226, 0.6255924105644226, 0.6303317546844482, 0.6255924105644226, 0.6255924105644226, 0.6303317546844482, 0.6255924105644226, 0.620853066444397, 0.6303317546844482, 0.6255924105644226, 0.6255924105644226, 0.6255924105644226, 0.6255924105644226, 0.6255924105644226, 0.6255924105644226, 0.6255924105644226, 0.6303317546844482, 0.6303317546844482, 0.6255924105644226, 0.6303317546844482, 0.6303317546844482, 0.6255924105644226, 0.6303317546844482, 0.6303317546844482, 0.6303317546844482, 0.6303317546844482, 0.6303317546844482, 0.6255924105644226, 0.6255924105644226, 0.6303317546844482, 0.6303317546844482, 0.6255924105644226, 0.6303317546844482, 0.6303317546844482, 0.6303317546844482, 0.6303317546844482, 0.6303317546844482, 0.6303317546844482, 0.6303317546844482, 0.6350710988044739, 0.6255924105644226, 0.6255924105644226, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6303317546844482, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6303317546844482, 0.6303317546844482, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6303317546844482, 0.6350710988044739, 0.6303317546844482, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6303317546844482, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6303317546844482, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6303317546844482, 0.6350710988044739, 0.6350710988044739, 0.6303317546844482, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6303317546844482, 0.6350710988044739, 0.6350710988044739, 0.6303317546844482, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6303317546844482, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6303317546844482, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739, 0.6350710988044739]}\n"
     ]
    }
   ],
   "source": [
    "### INICIE O CÓDIGO AQUI ### (1 linha de código)\n",
    "history = model.fit(train_set_X, train_set_Y, batch_size=64, epochs=1000)\n",
    "### TERMINE O CÓDIGO AQUI ###\n",
    "print(history.history)  # print per-epoch timeseries of metrics values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAÍDA ESPERADA**: <br>\n",
    "Na época 1000, tem-se o seguinte resultado (aproximado): <br>\n",
    "loss: 0.1938 - accuracy: 0.8062 - precision: 0.7644 - recall: 0.6303\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação do Modelo\n",
    "\n",
    "Avalie o desempenho da rede no conjunto de teste.\n",
    "\n",
    "Dica: use a função *evaluate*: https://keras.io/api/models/model_training_apis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 836us/step - loss: 0.2468 - accuracy: 0.7532 - precision: 0.7021 - recall: 0.5789\n",
      "Loss: 0.25 \n",
      "Accuracy: 0.75 \n",
      "Precision: 0.70 \n",
      "Recall: 0.58\n"
     ]
    }
   ],
   "source": [
    "### INICIE O CÓDIGO AQUI ### (1 linha de código)\n",
    "loss, acc, prec, rec = model.evaluate(test_set_X, test_set_Y)\n",
    "### TERMINE O CÓDIGO AQUI ###\n",
    "print(\"Loss: %.2f\" % loss, \"\\nAccuracy: %.2f\" % acc, \"\\nPrecision: %.2f\" % prec, \"\\nRecall: %.2f\" % rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAÍDA ESPERADA**: <br>\n",
    "Valores aproximados:\n",
    "\n",
    "Loss: 0.21  <br>\n",
    "Accuracy: 0.79  <br>\n",
    "Precision: 0.76  <br>\n",
    "Recall: 0.61\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predição\n",
    "\n",
    "Apresente a predição do conjunto de teste.\n",
    "\n",
    "Dica: use a função *predict*: https://keras.io/api/models/model_training_apis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:  [0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0]\n",
      "\n",
      "Correct:      [0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "### INICIE O CÓDIGO AQUI ### (1 linha de código)\n",
    "predictions = model.predict(test_set_X)\n",
    "### TERMINE O CÓDIGO AQUI ###\n",
    "print(\"Predictions: \", [round(x[0]) for x in predictions])\n",
    "print(\"\\nCorrect:     \", [round(x) for x in test_set_Y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAÍDA ESPERADA**: <br>\n",
    "Valores aproximados: <br>\n",
    "Predictions:  [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0]\n",
    "\n",
    "Correct:      [0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desafio\n",
    "\n",
    "Modifique a configuração da sua rede e/ou os parâmetros dos métodos com o objetivo de melhorar os resultados das métricas no conjunto de teste. Experimente várias opções. <br>\n",
    "Adicione abaixo a nova sequência de código que alcançou o melhor resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INICIE O CÓDIGO AQUI ### (várias linhas de código / várias células)\n",
    "\n",
    "# Configuração do modelo\n",
    "inputs = keras.Input(shape=(train_set_X.shape[1]))\n",
    "layer1 = keras.layers.Dense(units=3, activation='leaky_relu')(inputs)\n",
    "layer2 = keras.layers.Dense(units=5, activation='leaky_relu')(layer1)\n",
    "layer3 = keras.layers.Dense(units=3, activation='leaky_relu')(layer2)\n",
    "outputs = keras.layers.Dense(units=1, activation='sigmoid')(layer3)\n",
    "model = keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compilação do modelo\n",
    "model.compile(optimizer=\"sgd\", loss=\"mean_squared_error\", metrics=[\"accuracy\", keras.metrics.Precision(), keras.metrics.Recall()])\n",
    "\n",
    "# Resultado obtido após treinamento\n",
    "# Loss: 0.15 \n",
    "# Accuracy: 0.81 \n",
    "# Precision: 0.83 \n",
    "# Recall: 0.61\n",
    "\n",
    "### TERMINE O CÓDIGO AQUI ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fim\n",
    "\n",
    "Parabéns! Você efetuou todos os passos para criar uma rede neural com várias camadas para um problema de classificação binária.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
